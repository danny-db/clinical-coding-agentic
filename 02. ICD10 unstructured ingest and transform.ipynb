{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c756f50-2063-4a07-b964-e5d6de29abb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ“„ Create an unstructured data pipeline for gen AI retrievers\n",
    "\n",
    "This notebook shows you how to create a data pipeline that transforms unstructured documents into a vector index. By the end of this notebook, you will have a Databricks Vector Search index that an AI agent could use to power a retriever that queries information about unstructured data.\n",
    "\n",
    "This notebook creates a data pipeline using the following steps:\n",
    "1. Download sample PDF files from the GitHub repository [Databricks demo dataset](https://github.com/databricks-demos/dbdemos-dataset/tree/main).\n",
    "1. Load documents into a Delta table.\n",
    "1. Parse documents into text strings.\n",
    "1. Chunk the text strings into smaller, more manageable pieces for retrieval.\n",
    "1. Use an embedding model to embed the chunks into vectors and store the results in a vector index.\n",
    "\n",
    "To learn more about building and optimizing unstructured data pipelines, see Databricks documentation ([AWS](https://docs.databricks.com/aws/generative-ai/tutorials/ai-cookbook/quality-data-pipeline-rag) | [Azure](https://learn.microsoft.com/azure/databricks/generative-ai/tutorials/ai-cookbook/quality-data-pipeline-rag)).\n",
    "\n",
    "## Requirements\n",
    "\n",
    "This notebook requires Databricks Runtime Machine Learning version 14.3 and above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3777205-4dfe-418c-9d21-c67961a18070",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ‘‰ How to use this notebook\n",
    "\n",
    "Follow these steps to build and refine your data pipeline's quality:\n",
    "\n",
    "1. **Run this notebook to build a Vector Search index with default settings**\n",
    "    - Configure the data source and destination tables in the `1ï¸âƒ£ ðŸ“‚ Data source and destination configuration` cells\n",
    "    - Press `Run All` to create the vector index.\n",
    "\n",
    "    *Note: While you can adjust the other settings and modify the parsing/chunking code, we suggest doing so only after evaluating your Agent's quality so you can make improvements that specifically address root causes of quality issues.*\n",
    "\n",
    "2. **Run other sample notebooks to create an AI agent retriever that queries the vector index, then evaluate the agent/retriever's quality.**\n",
    "   - See agent examples that include boilerplate code to integrate a vector search index ([AWS](https://docs.databricks.com/aws/generative-ai/agent-framework/author-agent#chat-agent-examples) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/author-agent#chat-agent-examples)).\n",
    "\n",
    "3. **If the evaluation results show retrieval issues as a root cause, use this notebook to iterate on your data pipeline code & configuration.** \n",
    "\n",
    "    - The following are potential fixes you can try, see Databricks documentation for debugging retrieval issues for more information ([AWS](https://docs.databricks.com/aws/generative-ai/tutorials/ai-cookbook/implementation/step-5-debug-retrieval-quality) | [Azure](https://learn.microsoft.com/azure/databricks/generative-ai/tutorials/ai-cookbook/implementation/step-5-debug-retrieval-quality)).\n",
    "      - Add missing, but relevant source documents into in the index.\n",
    "      - Resolve any conflicting information in source documents.\n",
    "      - Adjust the data pipeline configuration:\n",
    "        - Modify chunk size or overlap.\n",
    "        - Experiment with different embedding models.\n",
    "      - Adjust the data pipeline code:\n",
    "        - Create a custom parser or use different parsing libraries.\n",
    "        - Develop a custom chunker or use different chunking techniques.\n",
    "        - Extract additional metadata for each document.\n",
    "      - Adjust the Agent's code/config in subsequent notebooks:\n",
    "        - Change the number of documents retrieved (K).\n",
    "        - Try a re-ranker.\n",
    "        - Use hybrid search.\n",
    "        - Apply extracted metadata as filters.\n",
    "\n",
    "**Note:** This notebook provides a foundation for creating unstructured data pipelines. For production workloads, Databricks recommends refactoring this notebook into separate components that can be orchestrated using [Databricks Workflows](https://www.databricks.com/product/workflows). In production workloads, you would pull out the code definitions into modules and separate the steps into individual tasks to be orchestrated over one or more workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a6053b9-3135-4097-9ed0-64bdb03a6b9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Important note:** Throughout this notebook, we indicate which cells you:\n",
    "- âœ… âœï¸ *should* customize - these cells contain code and config with business logic that you should edit to meet your requirements and tune quality\n",
    "- ðŸš« âœï¸ *typically should not* customize - these cells contain boilerplate code required to execute the pipeline\n",
    "\n",
    "Cells that don't require customization still need to be run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16b35cfd-7c99-4419-8978-33939faf24a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸš« âœï¸ Install Python libraries\n",
    "\n",
    "Only modify the following cells if you need additional packages in your code changes to the document parsing or chunking logic.\n",
    "\n",
    "Versions of Databricks code are not locked because Databricks ensures that changes are backward compatible.\n",
    "Versions of open source packages are locked because package authors often make breaking changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c325b01b-02a4-4ad6-a406-69a4ce21d07e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install libraries and restart Python"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U \\\n",
    "  \"pydantic>=2.9.2\" \\\n",
    "  \"mlflow>=2.18.0\" \\\n",
    "  \"databricks-sdk\" \\\n",
    "  \"databricks-vectorsearch\" \\\n",
    "  \"pymupdf4llm==0.0.5\" \\\n",
    "  \"pymupdf==1.24.13\" \\\n",
    "  \"markdownify==0.12.1\" \\\n",
    "  \"transformers==4.41.1\" \\\n",
    "  \"tiktoken==0.7.0\" \\\n",
    "  \"langchain-text-splitters==0.2.0\" \\\n",
    "  \"pypandoc_binary==1.13\" \\\n",
    "  \"pyyaml\"\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17ccaeea-166f-4171-87c0-35c493c0285f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸš« âœï¸ Define utility classes and functions\n",
    "\n",
    "Define utility functions. This is done to add modularization to the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a117f8a1-ee35-4214-8403-0750aa89d310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Serialization functions\n",
    "The goal of serialization is to save the class name (e.g., `util.xx.xx.configClassName`) with the dumped YAML.\n",
    "This allows ANY config to be dynamically loaded from a YAML without knowing about the `configClassName` before OR having it imported in your Python env.\n",
    "\n",
    "This is necessary for MultiAgent.`agents` and FunctionCallingAgent.`tools` since they can have multiple types of agent or tool configs in them -- when the config is loaded in the serving or local env, we don't know what these `configClassName` will be ahead of time & we want to avoid importing them all in the Python env.\n",
    "\n",
    "\n",
    "#### How it works:\n",
    "The ONLY way to dump a class is to call `model_dump()` on it, which will return a dict with the `_CLASS_PATH_KEY` key containing the class path e.g., `util.xx.xx.configClassName`.\n",
    "\n",
    "All other dumping methods (yaml, etc) call model_dump() since it is a Pydantic method. The ONLY way to load a serialized class is to call `load_obj_from_yaml` with the YAML string.\n",
    "`load_obj_from_yaml` will parse the YAML string and get the class path key.\n",
    "It will then use that class path key to dynamically load the class from the Python path.\n",
    "It will then call that class's _load_class_from_dict method with the remaining data to let it do anything custom e.g,. load the tools or the agents.\n",
    "\n",
    "If you haven't overridden `_load_class_from_dict`, it will call the default implementation of this method from `SerializableModel`\n",
    "otherwise, it will call your overridden `_load_class_from_dict` method.\n",
    "\n",
    "### How to use:\n",
    "Inherit your config class from `SerializableModel`.\n",
    "\n",
    "If you don't have any `SerializableModel` fields, you can call `load_obj_from_yaml` directly on your class's dumped YAML string; nothing else is required.\n",
    "\n",
    "If you have SerializableModel fields, you must:\n",
    "1. Override the _load_class_from_dict method to handle the deserialization of those sub-configs\n",
    "2. Override the model_dump method to call the model_dump of each of those sub-configs properly\n",
    "\n",
    "### Examples\n",
    "1. No sub-configs: GenieAgentConfig, UCTool\n",
    "2. Has sub-configs: FunctionCallingAgentConfig (in `tools`), MultiAgentConfig (in `agents`)\n",
    "load_obj_from_yaml --> The only way a class is loaded will get the classpath key\n",
    "\n",
    "TODO: add tests.  this was tested manually in a notebook verifying that all classes worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "861871a4-3b05-468c-a170-c73c276a57b9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define serialized config class and SDK helpers"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Tuple, Type\n",
    "import yaml\n",
    "from pydantic import BaseModel\n",
    "import importlib\n",
    "import json\n",
    "\n",
    "\n",
    "def serializable_config_to_yaml(obj: BaseModel) -> str:\n",
    "    data = obj.model_dump()\n",
    "    return yaml.dump(data)\n",
    "\n",
    "# TODO: add tests.  this was tested manually in a notebook verifying that all classes worked.\n",
    "\n",
    "\n",
    "_CLASS_PATH_KEY = \"class_path\"\n",
    "\n",
    "\n",
    "class SerializableConfig(BaseModel):\n",
    "    def to_yaml(self) -> str:\n",
    "        return serializable_config_to_yaml(self)\n",
    "\n",
    "    def model_dump(self, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Override model_dump to exclude name and description fields.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: Dictionary representation of the model excluding name and description.\n",
    "        \"\"\"\n",
    "        model_dumped = super().model_dump(**kwargs)\n",
    "        model_dumped[_CLASS_PATH_KEY] = f\"{self.__module__}.{self.__class__.__name__}\"\n",
    "        return model_dumped\n",
    "\n",
    "    @classmethod\n",
    "    def _load_class_from_dict(\n",
    "        cls, class_object, data: Dict[str, Any]\n",
    "    ) -> \"SerializableConfig\":\n",
    "        return class_object(**data)\n",
    "\n",
    "    def pretty_print(self):\n",
    "        print(json.dumps(self.model_dump(), indent=2))\n",
    "\n",
    "\n",
    "def serializable_config_to_yaml_file(obj: BaseModel, yaml_file_path: str) -> None:\n",
    "    with open(yaml_file_path, \"w\") as handle:\n",
    "        handle.write(serializable_config_to_yaml(obj))\n",
    "\n",
    "\n",
    "# Helper method used by SerializableModel's with fields containing SerializableModels\n",
    "def _load_class_from_dict(data: Dict[str, Any]) -> Tuple[Type, Dict[str, Any]]:\n",
    "    \"\"\"Dynamically load a class from data containing a class path.\n",
    "\n",
    "    Args:\n",
    "        data: Dictionary containing _CLASS_PATH_KEY and other data\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Type, Dict[str, Any]]: The class object and the remaining data\n",
    "    \"\"\"\n",
    "    class_path = data.pop(_CLASS_PATH_KEY)\n",
    "\n",
    "    module_name, class_name = class_path.rsplit(\".\", 1)\n",
    "    module = importlib.import_module(module_name)\n",
    "    return getattr(module, class_name), data\n",
    "\n",
    "\n",
    "def load_serializable_config_from_yaml(yaml_str: str) -> SerializableConfig:\n",
    "    data = yaml.safe_load(yaml_str)# Helper functions for displaying Delta Table and Volume URLs\n",
    "\n",
    "from typing import Optional\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from mlflow.utils import databricks_utils as du\n",
    "\n",
    "\n",
    "def get_databricks_cli_config() -> dict:\n",
    "    \"\"\"Retrieve the Databricks CLI configuration by running 'databricks auth describe' command.\n",
    "\n",
    "    Returns:\n",
    "        dict: The parsed JSON configuration from the Databricks CLI, or None if an error occurs\n",
    "\n",
    "    Note:\n",
    "        Requires the Databricks CLI to be installed and configured\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Run databricks auth describe command and capture output\n",
    "        process = subprocess.run(\n",
    "            [\"databricks\", \"auth\", \"describe\", \"-o\", \"json\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True,  # Raises CalledProcessError if command fails\n",
    "        )\n",
    "\n",
    "        # Parse JSON output\n",
    "        return json.loads(process.stdout)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error running databricks CLI command: {e}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing databricks CLI JSON output: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error getting databricks config from CLI: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_workspace_hostname() -> str:\n",
    "    \"\"\"Get the Databricks workspace hostname.\n",
    "\n",
    "    Returns:\n",
    "        str: The full workspace hostname (e.g., 'https://my-workspace.cloud.databricks.com')\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If not in a Databricks notebook and unable to get workspace hostname from CLI config\n",
    "    \"\"\"\n",
    "    if du.is_in_databricks_notebook():\n",
    "        return \"https://\" + du.get_browser_hostname()\n",
    "    else:\n",
    "        cli_config = get_databricks_cli_config()\n",
    "        if cli_config is None:\n",
    "            raise RuntimeError(\"Could not get Databricks CLI config\")\n",
    "        try:\n",
    "            return cli_config[\"details\"][\"host\"]\n",
    "        except KeyError:\n",
    "            raise RuntimeError(\n",
    "                \"Could not find workspace hostname in Databricks CLI config\"\n",
    "            )\n",
    "\n",
    "\n",
    "def get_table_url(table_fqdn: str) -> str:\n",
    "    \"\"\"Generate the URL for a Unity Catalog table in the Databricks UI.\n",
    "\n",
    "    Args:\n",
    "        table_fqdn: Fully qualified table name in format 'catalog.schema.table'.\n",
    "                   Can optionally include backticks around identifiers.\n",
    "\n",
    "    Returns:\n",
    "        str: The full URL to view the table in the Databricks UI.\n",
    "\n",
    "    Example:\n",
    "        >>> get_table_url(\"main.default.my_table\")\n",
    "        'https://my-workspace.cloud.databricks.com/explore/data/main/default/my_table'\n",
    "    \"\"\"\n",
    "    table_fqdn = table_fqdn.replace(\"`\", \"\")\n",
    "    catalog, schema, table = table_fqdn.split(\".\")\n",
    "    browser_url = get_workspace_hostname()\n",
    "    url = f\"{browser_url}/explore/data/{catalog}/{schema}/{table}\"\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_volume_url(volume_fqdn: str) -> str:\n",
    "    \"\"\"Generate the URL for a Unity Catalog volume in the Databricks UI.\n",
    "\n",
    "    Args:\n",
    "        volume_fqdn: Fully qualified volume name in format 'catalog.schema.volume'.\n",
    "                    Can optionally include backticks around identifiers.\n",
    "\n",
    "    Returns:\n",
    "        str: The full URL to view the volume in the Databricks UI.\n",
    "\n",
    "    Example:\n",
    "        >>> get_volume_url(\"main.default.my_volume\")\n",
    "        'https://my-workspace.cloud.databricks.com/explore/data/volumes/main/default/my_volume'\n",
    "    \"\"\"\n",
    "    volume_fqdn = volume_fqdn.replace(\"`\", \"\")\n",
    "    catalog, schema, volume = volume_fqdn.split(\".\")\n",
    "    browser_url = get_workspace_hostname()\n",
    "    url = f\"{browser_url}/explore/data/volumes/{catalog}/{schema}/{volume}\"\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_mlflow_experiment_url(experiment_id: str) -> str:\n",
    "    \"\"\"Generate the URL for an MLflow experiment in the Databricks UI.\n",
    "\n",
    "    Args:\n",
    "        experiment_id: The ID of the MLflow experiment\n",
    "\n",
    "    Returns:\n",
    "        str: The full URL to view the MLflow experiment in the Databricks UI.\n",
    "\n",
    "    Example:\n",
    "        >>> get_mlflow_experiment_url(\"<experiment_id>\")\n",
    "        'https://my-workspace.cloud.databricks.com/ml/experiments/<experiment_id>'\n",
    "    \"\"\"\n",
    "    browser_url = get_workspace_hostname()\n",
    "    url = f\"{browser_url}/ml/experiments/{experiment_id}\"\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_mlflow_experiment_traces_url(experiment_id: str) -> str:\n",
    "    \"\"\"Generate the URL for the MLflow experiment traces in the Databricks UI.\"\"\"\n",
    "    return get_mlflow_experiment_url(experiment_id) + \"?compareRunsMode=TRACES\"\n",
    "\n",
    "\n",
    "def get_function_url(function_fqdn: str) -> str:\n",
    "    \"\"\"Generate the URL for a Unity Catalog function in the Databricks UI.\n",
    "\n",
    "    Args:\n",
    "        function_fqdn: Fully qualified function name in format 'catalog.schema.function'.\n",
    "                      Can optionally include backticks around identifiers.\n",
    "\n",
    "    Returns:\n",
    "        str: The full URL to view the function in the Databricks UI.\n",
    "\n",
    "    Example:\n",
    "        >>> get_function_url(\"main.default.my_function\")\n",
    "        'https://my-workspace.cloud.databricks.com/explore/data/functions/main/default/my_function'\n",
    "    \"\"\"\n",
    "    function_fqdn = function_fqdn.replace(\"`\", \"\")\n",
    "    catalog, schema, function = function_fqdn.split(\".\")\n",
    "    browser_url = get_workspace_hostname()\n",
    "    url = f\"{browser_url}/explore/data/functions/{catalog}/{schema}/{function}\"\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_cluster_url(cluster_id: str) -> str:\n",
    "    \"\"\"Generate the URL for a Databricks cluster in the Databricks UI.\n",
    "\n",
    "    Args:\n",
    "        cluster_id: The ID of the cluster\n",
    "\n",
    "    Returns:\n",
    "        str: The full URL to view the cluster in the Databricks UI.\n",
    "\n",
    "    Example:\n",
    "        >>> get_cluster_url(\"<cluster_id>\")\n",
    "        'https://my-workspace.cloud.databricks.com/compute/clusters/<cluster_id>'\n",
    "    \"\"\"\n",
    "    browser_url = get_workspace_hostname()\n",
    "    url = f\"{browser_url}/compute/clusters/{cluster_id}\"\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_active_cluster_id_from_databricks_auth() -> Optional[str]:\n",
    "    \"\"\"Get the active cluster ID from the Databricks CLI authentication configuration.\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: The active cluster ID if found, None if not found or if an error occurs\n",
    "\n",
    "    Note:\n",
    "        This function relies on the Databricks CLI configuration having a cluster_id set\n",
    "    \"\"\"\n",
    "    if du.is_in_databricks_notebook():\n",
    "        raise ValueError(\n",
    "            \"Cannot get active cluster ID from the Databricks CLI in a Databricks notebook\"\n",
    "        )\n",
    "    try:\n",
    "        # Get config from the databricks cli\n",
    "        auth_output = get_databricks_cli_config()\n",
    "\n",
    "        # Safely navigate nested dict\n",
    "        details = auth_output.get(\"details\", {})\n",
    "        config = details.get(\"configuration\", {})\n",
    "        cluster = config.get(\"cluster_id\", {})\n",
    "        cluster_id = cluster.get(\"value\")\n",
    "\n",
    "        if cluster_id is None:\n",
    "            raise ValueError(\"Could not find cluster_id in Databricks auth config\")\n",
    "\n",
    "        return cluster_id\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_active_cluster_id() -> Optional[str]:\n",
    "    \"\"\"Get the active cluster ID.\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: The active cluster ID if found, None if not found or if an error occurs\n",
    "    \"\"\"\n",
    "    if du.is_in_databricks_notebook():\n",
    "        return du.get_active_cluster_id()\n",
    "    else:\n",
    "        return get_active_cluster_id_from_databricks_auth()\n",
    "\n",
    "\n",
    "def get_current_user_info(spark) -> tuple[str, str, str]:\n",
    "    # Get current user's name & email\n",
    "    w = WorkspaceClient()\n",
    "    user_email = w.current_user.me().user_name\n",
    "    user_name = user_email.split(\"@\")[0].replace(\".\", \"_\")\n",
    "\n",
    "    # Get the workspace default UC catalog\n",
    "    default_catalog = spark.sql(\"select current_catalog() as cur_catalog\").collect()[0][\n",
    "        \"cur_catalog\"\n",
    "    ]\n",
    "\n",
    "    return user_email, user_name, default_catalog\n",
    "\n",
    "    class_obj, remaining_data = _load_class_from_dict(data)\n",
    "    return class_obj._load_class_from_dict(class_obj, remaining_data)\n",
    "\n",
    "\n",
    "def load_serializable_config_from_yaml_file(yaml_file_path: str) -> SerializableConfig:\n",
    "    with open(yaml_file_path, \"r\") as file:\n",
    "        return load_serializable_config_from_yaml(file.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b3385d3-f3d2-4a90-ae11-ec1719b45f00",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define Unity Catalog volume source config class"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.errors import NotFound\n",
    "from databricks.sdk.errors.platform import ResourceAlreadyExists, ResourceDoesNotExist\n",
    "from databricks.sdk.service.catalog import VolumeType\n",
    "from pydantic import Field, computed_field, field_validator\n",
    "\n",
    "\n",
    "class UCVolumeSourceConfig(SerializableConfig):\n",
    "    \"\"\"\n",
    "    Source data configuration for the Unstructured Data Pipeline. You can modify this class to add additional configuration settings.\n",
    "\n",
    "    Args:\n",
    "      uc_catalog_name (str):\n",
    "        Required. Name of the Unity Catalog.\n",
    "      uc_schema_name (str):\n",
    "        Required. Name of the Unity Catalog schema.\n",
    "      uc_volume_name (str):\n",
    "        Required. Name of the Unity Catalog volume.\n",
    "    \"\"\"\n",
    "\n",
    "    @field_validator(\"uc_catalog_name\", \"uc_schema_name\", \"uc_volume_name\")\n",
    "    def validate_not_default(cls, value: str) -> str:\n",
    "        if value == \"REPLACE_ME\":\n",
    "            raise ValueError(\n",
    "                \"Please replace the default value 'REPLACE_ME' with your actual configuration\"\n",
    "            )\n",
    "        return value\n",
    "\n",
    "    uc_catalog_name: str = Field(..., min_length=1)\n",
    "    uc_schema_name: str = Field(..., min_length=1)\n",
    "    uc_volume_name: str = Field(..., min_length=1)\n",
    "\n",
    "    @computed_field()\n",
    "    def volume_path(self) -> str:\n",
    "        return f\"/Volumes/{self.uc_catalog_name}/{self.uc_schema_name}/{self.uc_volume_name}\"\n",
    "\n",
    "    @computed_field()\n",
    "    def volume_uc_fqn(self) -> str:\n",
    "        return f\"{self.uc_catalog_name}.{self.uc_schema_name}.{self.uc_volume_name}\"\n",
    "\n",
    "    def check_if_volume_exists(self) -> bool:\n",
    "        w = WorkspaceClient()\n",
    "        try:\n",
    "            # Use the computed field instead of reconstructing the FQN\n",
    "            w.volumes.read(name=self.volume_uc_fqn)\n",
    "            return True\n",
    "        except (ResourceDoesNotExist, NotFound):\n",
    "            return False\n",
    "\n",
    "    def create_volume(self):\n",
    "        try:\n",
    "            w = WorkspaceClient()\n",
    "            w.volumes.create(\n",
    "                catalog_name=self.uc_catalog_name,\n",
    "                schema_name=self.uc_schema_name,\n",
    "                name=self.uc_volume_name,\n",
    "                volume_type=VolumeType.MANAGED,\n",
    "            )\n",
    "        except ResourceAlreadyExists:\n",
    "            pass\n",
    "\n",
    "    def check_if_catalog_exists(self) -> bool:\n",
    "        w = WorkspaceClient()\n",
    "        try:\n",
    "            w.catalogs.get(name=self.uc_catalog_name)\n",
    "            return True\n",
    "        except (ResourceDoesNotExist, NotFound):\n",
    "            return False\n",
    "\n",
    "    def check_if_schema_exists(self) -> bool:\n",
    "        w = WorkspaceClient()\n",
    "        try:\n",
    "            full_name = f\"{self.uc_catalog_name}.{self.uc_schema_name}\"\n",
    "            w.schemas.get(full_name=full_name)\n",
    "            return True\n",
    "        except (ResourceDoesNotExist, NotFound):\n",
    "            return False\n",
    "\n",
    "    def create_or_validate_volume(self) -> tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Validates that the volume exists and creates it if it doesn't\n",
    "        Returns:\n",
    "            tuple[bool, str]: A tuple containing (success, error_message).\n",
    "            If validation passes, returns (True, success_message). If validation fails, returns (False, error_message).\n",
    "        \"\"\"\n",
    "        if not self.check_if_catalog_exists():\n",
    "            msg = f\"Catalog '{self.uc_catalog_name}' does not exist. Please create it first.\"\n",
    "            return (False, msg)\n",
    "\n",
    "        if not self.check_if_schema_exists():\n",
    "            msg = f\"Schema '{self.uc_schema_name}' does not exist in catalog '{self.uc_catalog_name}'. Please create it first.\"\n",
    "            return (False, msg)\n",
    "\n",
    "        if not self.check_if_volume_exists():\n",
    "            print(f\"Volume {self.volume_path} does not exist. Creating...\")\n",
    "            try:\n",
    "                self.create_volume()\n",
    "            except Exception as e:\n",
    "                msg = f\"Failed to create volume: {str(e)}\"\n",
    "                return (False, msg)\n",
    "            msg = f\"Successfully created volume {self.volume_path}. View here: {get_volume_url(self.volume_uc_fqn)}\"\n",
    "            print(msg)\n",
    "            return (True, msg)\n",
    "\n",
    "        msg = f\"Volume {self.volume_path} exists.  View here: {get_volume_url(self.volume_uc_fqn)}\"\n",
    "        print(msg)\n",
    "        return (True, msg)\n",
    "\n",
    "    def list_files(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Lists all files in the Unity Catalog volume using dbutils.fs.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: A list of file paths in the volume\n",
    "\n",
    "        Raises:\n",
    "            Exception: If the volume doesn't exist or there's an error accessing it\n",
    "        \"\"\"\n",
    "        if not self.check_if_volume_exists():\n",
    "            raise Exception(f\"Volume {self.volume_path} does not exist\")\n",
    "\n",
    "        w = WorkspaceClient()\n",
    "        try:\n",
    "            # List contents using dbutils.fs\n",
    "            files = w.dbutils.fs.ls(self.volume_path)\n",
    "            return [file.name for file in files]\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to list files in volume: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70097ac3-c5a8-493a-ad94-c075b11d85fa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define output config class and related helpers"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.errors import NotFound\n",
    "from databricks.sdk.errors.platform import ResourceDoesNotExist\n",
    "from databricks.sdk.service.vectorsearch import EndpointType\n",
    "\n",
    "\n",
    "class DataPipelineOutputConfig(SerializableConfig):\n",
    "    \"\"\"Configuration for managing output locations and naming conventions in the data pipeline.\n",
    "\n",
    "    This class handles the configuration of table names and vector search endpoints for the data pipeline.\n",
    "    It follows a consistent naming pattern for all generated tables and provides version control capabilities.\n",
    "\n",
    "    Naming Convention:\n",
    "        {catalog}.{schema}.{base_table_name}_{table_postfix}__{version_suffix}\n",
    "\n",
    "    Generated Tables:\n",
    "        1. Parsed docs table: Stores the raw parsed documents\n",
    "        2. Chunked docs table: Stores the documents split into chunks\n",
    "        3. Vector index: Stores the vector embeddings for search\n",
    "\n",
    "    Args:\n",
    "        uc_catalog_name (str): Unity Catalog name where tables will be created\n",
    "        uc_schema_name (str): Schema name within the catalog\n",
    "        base_table_name (str): Core name used as prefix for all generated tables\n",
    "        docs_table_postfix (str, optional): Suffix for the parsed documents table. Defaults to \"docs\"\n",
    "        chunked_table_postfix (str, optional): Suffix for the chunked documents table. Defaults to \"docs_chunked\"\n",
    "        vector_index_postfix (str, optional): Suffix for the vector index. Defaults to \"docs_chunked_index\"\n",
    "        version_suffix (str, optional): Version identifier (e.g., 'v1', 'test') to maintain multiple pipeline versions\n",
    "        vector_search_endpoint (str): Name of the vector search endpoint to use\n",
    "\n",
    "    Examples:\n",
    "        With version_suffix=\"v1\":\n",
    "            >>> config = DataPipelineOuputConfig(\n",
    "            ...     uc_catalog_name=\"my_catalog\",\n",
    "            ...     uc_schema_name=\"my_schema\",\n",
    "            ...     base_table_name=\"agent\",\n",
    "            ...     version_suffix=\"v1\"\n",
    "            ... )\n",
    "            # Generated tables:\n",
    "            # - my_catalog.my_schema.agent_docs__v1\n",
    "            # - my_catalog.my_schema.agent_docs_chunked__v1\n",
    "            # - my_catalog.my_schema.agent_docs_chunked_index__v1\n",
    "\n",
    "        Without version_suffix:\n",
    "            # - my_catalog.my_schema.agent_docs\n",
    "            # - my_catalog.my_schema.agent_docs_chunked\n",
    "            # - my_catalog.my_schema.agent_docs_chunked_index\n",
    "    \"\"\"\n",
    "\n",
    "    vector_search_endpoint: str\n",
    "    parsed_docs_table: str\n",
    "    chunked_docs_table: str\n",
    "    vector_index: str\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        vector_search_endpoint: str,\n",
    "        parsed_docs_table: Optional[str] = None,\n",
    "        chunked_docs_table: Optional[str] = None,\n",
    "        vector_index: Optional[str] = None,\n",
    "        uc_catalog_name: Optional[str] = None,\n",
    "        uc_schema_name: Optional[str] = None,\n",
    "        base_table_name: Optional[str] = None,\n",
    "        docs_table_postfix: str = \"docs\",\n",
    "        chunked_table_postfix: str = \"docs_chunked\",\n",
    "        vector_index_postfix: str = \"docs_chunked_index\",\n",
    "        version_suffix: Optional[str] = None,\n",
    "    ):\n",
    "        \"\"\"Initialize a new DataPipelineOuputConfig instance.\n",
    "\n",
    "        Supports two initialization styles:\n",
    "        1. Direct table names:\n",
    "            - parsed_docs_table\n",
    "            - chunked_docs_table\n",
    "            - vector_index\n",
    "\n",
    "        2. Generated table names using:\n",
    "            - uc_catalog_name\n",
    "            - uc_schema_name\n",
    "            - base_table_name\n",
    "            - [optional] postfixes and version_suffix\n",
    "\n",
    "        Args:\n",
    "            vector_search_endpoint (str): Name of the vector search endpoint to use\n",
    "            parsed_docs_table (str, optional): Direct table name for parsed docs\n",
    "            chunked_docs_table (str, optional): Direct table name for chunked docs\n",
    "            vector_index (str, optional): Direct name for vector index\n",
    "            uc_catalog_name (str, optional): Unity Catalog name where tables will be created\n",
    "            uc_schema_name (str, optional): Schema name within the catalog\n",
    "            base_table_name (str, optional): Core name used as prefix for all generated tables\n",
    "            docs_table_postfix (str, optional): Suffix for parsed documents table. Defaults to \"docs\"\n",
    "            chunked_table_postfix (str, optional): Suffix for chunked documents table. Defaults to \"docs_chunked\"\n",
    "            vector_index_postfix (str, optional): Suffix for vector index. Defaults to \"docs_chunked_index\"\n",
    "            version_suffix (str, optional): Version identifier for multiple pipeline versions\n",
    "        \"\"\"\n",
    "        _validate_not_default(vector_search_endpoint)\n",
    "\n",
    "        if parsed_docs_table and chunked_docs_table and vector_index:\n",
    "            # Direct table names provided\n",
    "            if any([uc_catalog_name, uc_schema_name, base_table_name]):\n",
    "                raise ValueError(\n",
    "                    \"Cannot provide both direct table names and table name generation parameters\"\n",
    "                )\n",
    "        elif all([uc_catalog_name, uc_schema_name, base_table_name]):\n",
    "            # Generate table names\n",
    "            _validate_not_default(uc_catalog_name)\n",
    "            _validate_not_default(uc_schema_name)\n",
    "            _validate_not_default(base_table_name)\n",
    "\n",
    "            parsed_docs_table = _build_table_name(\n",
    "                uc_catalog_name,\n",
    "                uc_schema_name,\n",
    "                base_table_name,\n",
    "                docs_table_postfix,\n",
    "                version_suffix,\n",
    "            )\n",
    "            chunked_docs_table = _build_table_name(\n",
    "                uc_catalog_name,\n",
    "                uc_schema_name,\n",
    "                base_table_name,\n",
    "                chunked_table_postfix,\n",
    "                version_suffix,\n",
    "            )\n",
    "            vector_index = _build_table_name(\n",
    "                uc_catalog_name,\n",
    "                uc_schema_name,\n",
    "                base_table_name,\n",
    "                vector_index_postfix,\n",
    "                version_suffix,\n",
    "                escape=False,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Must provide either all direct table names or all table name generation parameters\"\n",
    "            )\n",
    "\n",
    "        super().__init__(\n",
    "            parsed_docs_table=parsed_docs_table,\n",
    "            chunked_docs_table=chunked_docs_table,\n",
    "            vector_index=vector_index,\n",
    "            vector_search_endpoint=vector_search_endpoint,\n",
    "        )\n",
    "\n",
    "    def check_if_vector_search_endpoint_exists(self):\n",
    "        w = WorkspaceClient()\n",
    "        vector_search_endpoints = w.vector_search_endpoints.list_endpoints()\n",
    "        if (\n",
    "            sum(\n",
    "                [\n",
    "                    self.vector_search_endpoint == ve.name\n",
    "                    for ve in vector_search_endpoints\n",
    "                ]\n",
    "            )\n",
    "            == 0\n",
    "        ):\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def create_vector_search_endpoint(self):\n",
    "        w = WorkspaceClient()\n",
    "        print(\n",
    "            f\"Please wait, creating Vector Search endpoint `{self.vector_search_endpoint}`.  This can take up to 20 minutes...\"\n",
    "        )\n",
    "        w.vector_search_endpoints.create_endpoint_and_wait(\n",
    "            self.vector_search_endpoint, endpoint_type=EndpointType.STANDARD\n",
    "        )\n",
    "        # Make sure vector search endpoint is online and ready.\n",
    "        w.vector_search_endpoints.wait_get_endpoint_vector_search_endpoint_online(\n",
    "            self.vector_search_endpoint\n",
    "        )\n",
    "\n",
    "    def create_or_validate_vector_search_endpoint(self):\n",
    "        if not self.check_if_vector_search_endpoint_exists():\n",
    "            self.create_vector_search_endpoint()\n",
    "        return self.validate_vector_search_endpoint()\n",
    "\n",
    "    def validate_vector_search_endpoint(self) -> tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Validates that the specified Vector Search endpoint exists\n",
    "        Returns:\n",
    "            tuple[bool, str]: A tuple containing (success, error_message).\n",
    "            If validation passes, returns (True, success_message). If validation fails, returns (False, error_message).\n",
    "        \"\"\"\n",
    "        if not self.check_if_vector_search_endpoint_exists():\n",
    "            msg = f\"Vector Search endpoint '{self.vector_search_endpoint}' does not exist. Please either manually create it or call `output_config.create_or_validate_vector_search_endpoint()` to create it.\"\n",
    "            return (False, msg)\n",
    "\n",
    "        msg = f\"Vector Search endpoint '{self.vector_search_endpoint}' exists.\"\n",
    "        print(msg)\n",
    "        return (True, msg)\n",
    "\n",
    "    def validate_catalog_and_schema(self) -> tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Validates that the specified catalog and schema exist\n",
    "        Returns:\n",
    "            tuple[bool, str]: A tuple containing (success, error_message).\n",
    "            If validation passes, returns (True, success_message). If validation fails, returns (False, error_message).\n",
    "        \"\"\"\n",
    "\n",
    "        # Check catalog and schema for parsed_docs_table\n",
    "        parsed_docs_catalog = _get_uc_catalog_name(self.parsed_docs_table)\n",
    "        parsed_docs_schema = _get_uc_schema_name(self.parsed_docs_table)\n",
    "        if not _check_if_catalog_exists(parsed_docs_catalog):\n",
    "            msg = f\"Catalog '{parsed_docs_catalog}' does not exist for parsed_docs_table. Please create it first.\"\n",
    "            return (False, msg)\n",
    "        if not _check_if_schema_exists(parsed_docs_catalog, parsed_docs_schema):\n",
    "            msg = f\"Schema '{parsed_docs_schema}' does not exist in catalog '{parsed_docs_catalog}' for parsed_docs_table. Please create it first.\"\n",
    "            return (False, msg)\n",
    "\n",
    "        # Check catalog and schema for chunked_docs_table\n",
    "        chunked_docs_catalog = _get_uc_catalog_name(self.chunked_docs_table)\n",
    "        chunked_docs_schema = _get_uc_schema_name(self.chunked_docs_table)\n",
    "        if not _check_if_catalog_exists(chunked_docs_catalog):\n",
    "            msg = f\"Catalog '{chunked_docs_catalog}' does not exist for chunked_docs_table. Please create it first.\"\n",
    "            return (False, msg)\n",
    "        if not _check_if_schema_exists(chunked_docs_catalog, chunked_docs_schema):\n",
    "            msg = f\"Schema '{chunked_docs_schema}' does not exist in catalog '{chunked_docs_catalog}' for chunked_docs_table. Please create it first.\"\n",
    "            return (False, msg)\n",
    "\n",
    "        # Check catalog and schema for vector_index\n",
    "        vector_index_catalog = _get_uc_catalog_name(self.vector_index)\n",
    "        vector_index_schema = _get_uc_schema_name(self.vector_index)\n",
    "        if not _check_if_catalog_exists(vector_index_catalog):\n",
    "            msg = f\"Catalog '{vector_index_catalog}' does not exist for vector_index. Please create it first.\"\n",
    "            return (False, msg)\n",
    "        if not _check_if_schema_exists(vector_index_catalog, vector_index_schema):\n",
    "            msg = f\"Schema '{vector_index_schema}' does not exist in catalog '{vector_index_catalog}' for vector_index. Please create it first.\"\n",
    "            return (False, msg)\n",
    "\n",
    "        msg = f\"All catalogs and schemas exist for parsed_docs_table, chunked_docs_table, and vector_index.\"\n",
    "        print(msg)\n",
    "        return (True, msg)\n",
    "\n",
    "\n",
    "def _escape_uc_fqn(uc_fqn: str) -> str:\n",
    "    \"\"\"\n",
    "    Escape the fully qualified name (FQN) for a Unity Catalog asset if it contains special characters.\n",
    "\n",
    "    Args:\n",
    "        uc_fqn (str): The fully qualified name of the asset.\n",
    "\n",
    "    Returns:\n",
    "        str: The escaped fully qualified name if it contains special characters, otherwise the original FQN.\n",
    "    \"\"\"\n",
    "    if \"-\" in uc_fqn:\n",
    "        parts = uc_fqn.split(\".\")\n",
    "        escaped_parts = [f\"`{part}`\" for part in parts]\n",
    "        return \".\".join(escaped_parts)\n",
    "    else:\n",
    "        return uc_fqn\n",
    "\n",
    "\n",
    "def _build_table_name(\n",
    "    uc_catalog_name: str,\n",
    "    uc_schema_name: str,\n",
    "    base_table_name: str,\n",
    "    postfix: str,\n",
    "    version_suffix: str = None,\n",
    "    escape: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"Helper to build consistent table names\n",
    "\n",
    "    Args:\n",
    "        postfix: The table name postfix to append\n",
    "        escape: Whether to escape special characters in the table name. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        The constructed table name, optionally escaped\n",
    "    \"\"\"\n",
    "    suffix = f\"__{version_suffix}\" if version_suffix else \"\"\n",
    "    raw_name = f\"{uc_catalog_name}.{uc_schema_name}.{base_table_name}_{postfix}{suffix}\"\n",
    "    return _escape_uc_fqn(raw_name) if escape else raw_name\n",
    "\n",
    "\n",
    "def _validate_not_default(value: str) -> str:\n",
    "    if value == \"REPLACE_ME\":\n",
    "        raise ValueError(\n",
    "            \"Please replace the default value 'REPLACE_ME' with your actual configuration\"\n",
    "        )\n",
    "    return value\n",
    "\n",
    "\n",
    "def _get_uc_catalog_name(uc_fqn: str) -> str:\n",
    "    unescaped_uc_fqn = uc_fqn.replace(\"`\", \"\")\n",
    "    return unescaped_uc_fqn.split(\".\")[0]\n",
    "\n",
    "\n",
    "def _get_uc_schema_name(uc_fqn: str) -> str:\n",
    "    unescaped_uc_fqn = uc_fqn.replace(\"`\", \"\")\n",
    "    return unescaped_uc_fqn.split(\".\")[1]\n",
    "\n",
    "\n",
    "def _check_if_catalog_exists(uc_catalog_name) -> bool:\n",
    "    w = WorkspaceClient()\n",
    "    try:\n",
    "        w.catalogs.get(name=uc_catalog_name)\n",
    "        return True\n",
    "    except (ResourceDoesNotExist, NotFound):\n",
    "        return False\n",
    "\n",
    "\n",
    "def _check_if_schema_exists(uc_catalog_name, uc_schema_name) -> bool:\n",
    "    w = WorkspaceClient()\n",
    "    try:\n",
    "        full_name = f\"{uc_catalog_name}.{uc_schema_name}\"\n",
    "        w.schemas.get(full_name=full_name)\n",
    "        return True\n",
    "    except (ResourceDoesNotExist, NotFound):\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02d4e4d7-6c87-4d89-a86e-91cfb7446b2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import collections\n",
    "import os\n",
    "\n",
    "\n",
    "def download_file_from_git(dest, owner, repo, path):\n",
    "    def download_file(url, destination):\n",
    "        local_filename = url.split(\"/\")[-1]\n",
    "        # NOTE the stream=True parameter below\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            print(\"saving \" + destination + \"/\" + local_filename)\n",
    "            with open(destination + \"/\" + local_filename, \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    # If you have chunk encoded response uncomment if\n",
    "                    # and set chunk_size parameter to None.\n",
    "                    # if chunk:\n",
    "                    f.write(chunk)\n",
    "        return local_filename\n",
    "\n",
    "    if not os.path.exists(dest):\n",
    "        os.makedirs(dest)\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "    files = requests.get(\n",
    "        f\"https://api.github.com/repos/{owner}/{repo}/contents{path}\"\n",
    "    ).json()\n",
    "    files = [f[\"download_url\"] for f in files if \"NOTICE\" not in f[\"name\"]]\n",
    "\n",
    "    def download_to_dest(url):\n",
    "        download_file(url, dest)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        collections.deque(executor.map(download_to_dest, files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f777a710-958a-4ee8-85e3-0780dd286b04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ“‚ Data source & destination configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f54faab-c83a-4a8f-a8b2-91bb1c241816",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## âœ… âœï¸ Configure the data pipeline's source location.\n",
    "\n",
    "Choose a [Unity Catalog Volume](https://docs.databricks.com/en/volumes/index.html) containing PDF, HTML, etc... documents to be parsed, chunked, and embedded.\n",
    "\n",
    "Use the widgets at the top of the notebook to choose the following values:\n",
    "\n",
    "- `uc_catalog_name`: Name of the Unity Catalog.\n",
    "- `uc_schema_name`: Name of the Unity Catalog schema.\n",
    "- `uc_volume_name`: Name of the Unity Catalog volume.\n",
    "\n",
    "Running these cells will validate that the Unity Catalog Volume exists and try to create it if it does not.\n",
    "\n",
    "The code in this section is organized around a class to represent the Unity Catalog Volume as a source for your data pipeline and an associated parent class for managing serializable configuration objects. The primary cell to focus on is the one that configures and validates the source object, **Configure and create or validate the volume**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1395658d-3074-4606-a5a0-97b4fba7756c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dbutils.widgets.text(\"db_name\",'',label=\"Database\")\n",
    "dbutils.widgets.text(\"catalog\", '',label=\"Catalog\")\n",
    "dbutils.widgets.text(\"volume_name\", '',label=\"Volume Name\")\n",
    "\n",
    "uc_catalog_name = dbutils.widgets.get(\"catalog\")\n",
    "uc_schema_name = dbutils.widgets.get(\"db_name\")\n",
    "uc_volume_name = dbutils.widgets.get(\"volume_name\")\n",
    "\n",
    "if not uc_catalog_name or not uc_schema_name  or not uc_volume_name:\n",
    "  print(\"Please set all the Data Configurations\")\n",
    "\n",
    "\n",
    "\n",
    "# Configure the Unity Catalog Volume that contains the source documents\n",
    "source_config = UCVolumeSourceConfig(\n",
    "  uc_catalog_name = dbutils.widgets.get(\"catalog\"),\n",
    "  uc_schema_name = dbutils.widgets.get(\"db_name\"),\n",
    "  uc_volume_name = dbutils.widgets.get(\"volume_name\")\n",
    ")\n",
    "\n",
    "# Check if volume exists, create otherwise\n",
    "is_valid, msg = source_config.create_or_validate_volume()\n",
    "if not is_valid:\n",
    "    raise Exception(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af790b68-ff64-40af-8b4d-7ece105018b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### You can upload your own ICD-10 pdf files to the UC volume.  Or you can keep the UC volume empty and copy the files from the repo (That includes both ICD-10 and ICD-10-AM definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ca57de6-3188-476c-ac82-1b95a3367ce2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get the pdfs from a remote location"
    }
   },
   "outputs": [],
   "source": [
    "volume_path =  f'/Volumes/{uc_catalog_name}/{uc_schema_name}/{uc_volume_name}'\n",
    "\n",
    "owner = \"danny-db\"\n",
    "repo = \"clinical-coding-agentic\"\n",
    "path =  \"/icd10-pdf\"\n",
    "files = dbutils.fs.ls(volume_path)\n",
    "\n",
    "if not files:\n",
    "    download_file_from_git(volume_path, owner, repo, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "083e203f-e468-4ce7-b645-31507a36c86b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## âœ… âœï¸ Configure the data pipeline's output location.\n",
    " \n",
    "Choose where the data pipeline outputs the parsed, chunked, and embedded documents.\n",
    "\n",
    "Required parameters:\n",
    "* `uc_catalog_name`: Unity Catalog name where tables will be created\n",
    "* `uc_schema_name`: Schema name in the catalog \n",
    "* `base_table_name`: Core name used as a prefix for all generated tables\n",
    "* `vector_search_endpoint`: Vector Search endpoint to store the index\n",
    "\n",
    "Optional parameters:\n",
    "* `docs_table_postfix`: Suffix for the parsed documents table (default: \"docs\")\n",
    "* `chunked_table_postfix`: Suffix for the chunked documents table (default: \"docs_chunked\") \n",
    "* `vector_index_postfix`: Suffix for the vector index (default: \"docs_chunked_index\")\n",
    "* `version_suffix`: Version identifier (e.g., 'v1', 'test') to maintain multiple versions\n",
    "\n",
    "The generated tables follow this naming convention:\n",
    "* Parsed docs: {uc_catalog_name}.{uc_schema_name}.{base_table_name}_{docs_table_postfix}__{version_suffix}\n",
    "* Chunked docs: {uc_catalog_name}.{uc_schema_name}.{base_table_name}_{chunked_table_postfix}__{version_suffix}\n",
    "* Vector index: {uc_catalog_name}.{uc_schema_name}.{base_table_name}_{vector_index_postfix}__{version_suffix}\n",
    "\n",
    "*Note: If you are comparing different chunking/parsing/embedding strategies, set the `version_suffix` parameter to maintain multiple versions of the pipeline output with the same base_table_name.*\n",
    "\n",
    "*Databricks suggests sharing a Vector Search endpoint across multiple agents.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cba85eb2-64ea-4c81-afd7-38489698baea",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create and validate the output config"
    }
   },
   "outputs": [],
   "source": [
    "# Output configuration\n",
    "output_config = DataPipelineOutputConfig(\n",
    "    # Required parameters\n",
    "    uc_catalog_name=source_config.uc_catalog_name, # usually same as source volume catalog, by default is the same as the source volume catalog\n",
    "    uc_schema_name=source_config.uc_schema_name, # usually same as source volume schema, by default is the same as the source volume schema\n",
    "    base_table_name=source_config.uc_volume_name, # usually similar / same as the source volume name; by default, is the same as the volume_name\n",
    "    # vector_search_endpoint=\"REPLACE_ME\", # Vector Search endpoint to store the index\n",
    "    vector_search_endpoint=\"one-env-shared-endpoint-8\", # Vector Search endpoint to store the index #DW (better change it)\n",
    "\n",
    "    # Optional parameters, showing defaults\n",
    "    docs_table_postfix=\"docs\",              # default value is `docs`\n",
    "    chunked_table_postfix=\"docs_chunked\",   # default value is `docs_chunked`\n",
    "    vector_index_postfix=\"docs_chunked_index\", # default value is `docs_chunked_index`\n",
    "    version_suffix= None                     # default is None\n",
    "\n",
    "    # Output tables / indexes follow this naming convention:\n",
    "    # {uc_catalog_name}.{uc_schema_name}.{base_table_name}_{docs_table_postfix}__{version_suffix}\n",
    "    # {uc_catalog_name}.{uc_schema_name}.{base_table_name}_{chunked_table_postfix}__{version_suffix}\n",
    "    # {uc_catalog_name}.{uc_schema_name}.{base_table_name}_{vector_index_postfix}__{version_suffix}\n",
    ")\n",
    "\n",
    "# Alternatively, you can directly pass in the Unity Catalog locations of the tables / indexes\n",
    "# output_config = DataPipelineOutputConfig(\n",
    "#     chunked_docs_table=\"catalog.schema.docs_chunked\",\n",
    "#     parsed_docs_table=\"catalog.schema.parsed_docs\",\n",
    "#     vector_index=\"catalog.schema.docs_chunked_index\",\n",
    "#     vector_search_endpoint=\"REPLACE_ME\",\n",
    "# )\n",
    "\n",
    "# Check Unity Catalog locations exists\n",
    "is_valid, msg = output_config.validate_catalog_and_schema()\n",
    "if not is_valid:\n",
    "    raise Exception(msg)\n",
    "\n",
    "# Check Vector Search endpoint exists\n",
    "is_valid, msg = output_config.create_or_validate_vector_search_endpoint()\n",
    "if not is_valid:\n",
    "    raise Exception(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5b380e5-1d9a-4c93-b8fe-ec23f00442a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## âœ… âœï¸ Configure chunk size and embedding model\n",
    "\n",
    "**Chunk size and overlap** control how a larger document is turned into smaller chunks that an embedding model can process.  See Databricks documentation - Chunking for more information ([AWS](https://docs.databricks.com/aws/generative-ai/tutorials/ai-cookbook/quality-data-pipeline-rag#chunking) | [Azure](https://learn.microsoft.com/azure/databricks/generative-ai/tutorials/ai-cookbook/quality-data-pipeline-rag#chunking))\n",
    "\n",
    "**The embedding model** is an AI model that identifies the most similar documents to a user's query.  See Databricks documentation - Embedding model for more details ([AWS](https://docs.databricks.com/aws/generative-ai/tutorials/ai-cookbook/quality-data-pipeline-rag#embedding-model) | [Azure](https://learn.microsoft.com/azure/databricks/generative-ai/tutorials/ai-cookbook/quality-data-pipeline-rag#embedding-model)).\n",
    "\n",
    "This notebook supports the following [Foundational Models](https://docs.databricks.com/en/machine-learning/foundation-models/index.html) or [External Model](https://docs.databricks.com/en/generative-ai/external-models/index.html) of type `/llm/v1/embeddings`/.  If you want to try another model, you must modify `utils/get_recursive_character_text_splitter` to add support.\n",
    "- `databricks-gte-large-en` or `databricks-bge-large-en`\n",
    "- Azure OpenAI or OpenAI External Model of type `text-embedding-ada-002`, `text-embedding-3-small` or `text-embedding-3-large`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ab85bca-2a92-4d71-9adc-6a9b31359dcd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define chunking and embedding helpers"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Callable, Tuple, Optional\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Constants\n",
    "HF_CACHE_DIR = \"/local_disk0/tmp/hf_cache/\"\n",
    "\n",
    "# Embedding Models Configuration\n",
    "EMBEDDING_MODELS = {\n",
    "    \"gte-large-en-v1.5\": {\n",
    "        # \"tokenizer\": lambda: AutoTokenizer.from_pretrained(\n",
    "        #     \"Alibaba-NLP/gte-large-en-v1.5\", cache_dir=HF_CACHE_DIR\n",
    "        # ),\n",
    "        \"context_window\": 8192,\n",
    "        \"type\": \"SENTENCE_TRANSFORMER\",\n",
    "    },\n",
    "    \"bge-large-en-v1.5\": {\n",
    "        # \"tokenizer\": lambda: AutoTokenizer.from_pretrained(\n",
    "        #     \"BAAI/bge-large-en-v1.5\", cache_dir=HF_CACHE_DIR\n",
    "        # ),\n",
    "        \"context_window\": 512,\n",
    "        \"type\": \"SENTENCE_TRANSFORMER\",\n",
    "    },\n",
    "    \"bge_large_en_v1_5\": {\n",
    "        # \"tokenizer\": lambda: AutoTokenizer.from_pretrained(\n",
    "        #     \"BAAI/bge-large-en-v1.5\", cache_dir=HF_CACHE_DIR\n",
    "        # ),\n",
    "        \"context_window\": 512,\n",
    "        \"type\": \"SENTENCE_TRANSFORMER\",\n",
    "    },\n",
    "    \"text-embedding-ada-002\": {\n",
    "        \"context_window\": 8192,\n",
    "        # \"tokenizer\": lambda: tiktoken.encoding_for_model(\"text-embedding-ada-002\"),\n",
    "        \"type\": \"OPENAI\",\n",
    "    },\n",
    "    \"text-embedding-3-small\": {\n",
    "        \"context_window\": 8192,\n",
    "        # \"tokenizer\": lambda: tiktoken.encoding_for_model(\"text-embedding-3-small\"),\n",
    "        \"type\": \"OPENAI\",\n",
    "    },\n",
    "    \"text-embedding-3-large\": {\n",
    "        \"context_window\": 8192,\n",
    "        # \"tokenizer\": lambda: tiktoken.encoding_for_model(\"text-embedding-3-large\"),\n",
    "        \"type\": \"OPENAI\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def get_workspace_client() -> WorkspaceClient:\n",
    "    \"\"\"Returns a WorkspaceClient instance.\"\"\"\n",
    "    return WorkspaceClient()\n",
    "\n",
    "\n",
    "# TODO: this is a cheap hack to avoid importing tokenizer libs at the top level -  the datapipeline utils are imported by the agent notebook which won't have these libs loaded & we don't want to since autotokenizer is heavy weight.\n",
    "def get_embedding_model_tokenizer(endpoint_type: str) -> Optional[dict]:\n",
    "    from transformers import AutoTokenizer\n",
    "    import tiktoken\n",
    "\n",
    "    # copy here to prevent needing to install tokenizer libraries everywhere this is imported\n",
    "    EMBEDDING_MODELS_W_TOKENIZER = {\n",
    "        \"gte-large-en-v1.5\": {\n",
    "            \"tokenizer\": lambda: AutoTokenizer.from_pretrained(\n",
    "                \"Alibaba-NLP/gte-large-en-v1.5\", cache_dir=HF_CACHE_DIR\n",
    "            ),\n",
    "            \"context_window\": 8192,\n",
    "            \"type\": \"SENTENCE_TRANSFORMER\",\n",
    "        },\n",
    "        \"bge-large-en-v1.5\": {\n",
    "            \"tokenizer\": lambda: AutoTokenizer.from_pretrained(\n",
    "                \"BAAI/bge-large-en-v1.5\", cache_dir=HF_CACHE_DIR\n",
    "            ),\n",
    "            \"context_window\": 512,\n",
    "            \"type\": \"SENTENCE_TRANSFORMER\",\n",
    "        },\n",
    "        \"bge_large_en_v1_5\": {\n",
    "            \"tokenizer\": lambda: AutoTokenizer.from_pretrained(\n",
    "                \"BAAI/bge-large-en-v1.5\", cache_dir=HF_CACHE_DIR\n",
    "            ),\n",
    "            \"context_window\": 512,\n",
    "            \"type\": \"SENTENCE_TRANSFORMER\",\n",
    "        },\n",
    "        \"text-embedding-ada-002\": {\n",
    "            \"context_window\": 8192,\n",
    "            \"tokenizer\": lambda: tiktoken.encoding_for_model(\"text-embedding-ada-002\"),\n",
    "            \"type\": \"OPENAI\",\n",
    "        },\n",
    "        \"text-embedding-3-small\": {\n",
    "            \"context_window\": 8192,\n",
    "            \"tokenizer\": lambda: tiktoken.encoding_for_model(\"text-embedding-3-small\"),\n",
    "            \"type\": \"OPENAI\",\n",
    "        },\n",
    "        \"text-embedding-3-large\": {\n",
    "            \"context_window\": 8192,\n",
    "            \"tokenizer\": lambda: tiktoken.encoding_for_model(\"text-embedding-3-large\"),\n",
    "            \"type\": \"OPENAI\",\n",
    "        },\n",
    "    }\n",
    "    return EMBEDDING_MODELS_W_TOKENIZER.get(endpoint_type).get(\"tokenizer\")\n",
    "\n",
    "\n",
    "def get_embedding_model_config(endpoint_type: str) -> Optional[dict]:\n",
    "    \"\"\"\n",
    "    Retrieve embedding model configuration by endpoint type.\n",
    "    \"\"\"\n",
    "\n",
    "    return EMBEDDING_MODELS.get(endpoint_type)\n",
    "\n",
    "\n",
    "def extract_endpoint_type(llm_endpoint) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract the endpoint type from the given llm_endpoint object.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return llm_endpoint.config.served_entities[0].external_model.name\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            return llm_endpoint.config.served_entities[0].foundation_model.name\n",
    "        except AttributeError:\n",
    "            return None\n",
    "\n",
    "\n",
    "def detect_fmapi_embedding_model_type(\n",
    "    model_serving_endpoint: str,\n",
    ") -> Tuple[Optional[str], Optional[dict]]:\n",
    "    \"\"\"\n",
    "    Detects the embedding model type and configuration for the given endpoint.\n",
    "    Returns a tuple of (endpoint_type, embedding_config) or (None, None) if not found.\n",
    "    \"\"\"\n",
    "    client = get_workspace_client()\n",
    "\n",
    "    try:\n",
    "        llm_endpoint = client.serving_endpoints.get(name=model_serving_endpoint)\n",
    "        endpoint_type = extract_endpoint_type(llm_endpoint)\n",
    "    except Exception as e:\n",
    "        endpoint_type = None\n",
    "\n",
    "    embedding_config = (\n",
    "        get_embedding_model_config(endpoint_type) if endpoint_type else None\n",
    "    )\n",
    "\n",
    "    embedding_config[\"tokenizer\"] = (\n",
    "        get_embedding_model_tokenizer(endpoint_type) if endpoint_type else None\n",
    "    )\n",
    "\n",
    "    return (endpoint_type, embedding_config)\n",
    "\n",
    "\n",
    "def validate_chunk_size(chunk_spec: dict):\n",
    "    \"\"\"\n",
    "    Validate the chunk size and overlap settings in chunk_spec.\n",
    "    Raises ValueError if any condition is violated.\n",
    "    \"\"\"\n",
    "    if (\n",
    "        chunk_spec[\"chunk_overlap_tokens\"] + chunk_spec[\"chunk_size_tokens\"]\n",
    "    ) > chunk_spec[\"context_window\"]:\n",
    "        msg = (\n",
    "            f'Proposed chunk_size of {chunk_spec[\"chunk_size_tokens\"]} + overlap of {chunk_spec[\"chunk_overlap_tokens\"]} '\n",
    "            f'is {chunk_spec[\"chunk_overlap_tokens\"] + chunk_spec[\"chunk_size_tokens\"]} which is greater than context '\n",
    "            f'window of {chunk_spec[\"context_window\"]} tokens.',\n",
    "        )\n",
    "        return (False, msg)\n",
    "    elif chunk_spec[\"chunk_overlap_tokens\"] > chunk_spec[\"chunk_size_tokens\"]:\n",
    "        msg = (\n",
    "            f'Proposed `chunk_overlap_tokens` of {chunk_spec[\"chunk_overlap_tokens\"]} is greater than the '\n",
    "            f'`chunk_size_tokens` of {chunk_spec[\"chunk_size_tokens\"]}. Reduce the size of `chunk_size_tokens`.',\n",
    "        )\n",
    "        return (False, msg)\n",
    "    else:\n",
    "        context_usage = (\n",
    "            round(\n",
    "                (chunk_spec[\"chunk_size_tokens\"] + chunk_spec[\"chunk_overlap_tokens\"])\n",
    "                / chunk_spec[\"context_window\"],\n",
    "                2,\n",
    "            )\n",
    "            * 100\n",
    "        )\n",
    "        msg = f'Chunk size in tokens: {chunk_spec[\"chunk_size_tokens\"]} and chunk overlap in tokens: {chunk_spec[\"chunk_overlap_tokens\"]} are valid.  Using {round(context_usage, 2)}% ({chunk_spec[\"chunk_size_tokens\"] + chunk_spec[\"chunk_overlap_tokens\"]} tokens) of the {chunk_spec[\"context_window\"]} token context window.'\n",
    "        return (True, msg)\n",
    "\n",
    "def get_recursive_character_text_splitter(\n",
    "    model_serving_endpoint: str,\n",
    "    embedding_model_name: str = None,\n",
    "    chunk_size_tokens: int = None,\n",
    "    chunk_overlap_tokens: int = 0,\n",
    ") -> Callable[[str], list[str]]:\n",
    "    \"\"\"\n",
    "    Creates a new function that, given an embedding endpoint, returns a callable that can chunk text documents. This utility allows you to write the core business logic of the chunker, without dealing with the details of text splitting. You can decide to write your own, or edit this code if it does not fit your use case.\n",
    "\n",
    "    Args:\n",
    "        model_serving_endpoint (str):\n",
    "            The name of the Model Serving endpoint with the embedding model.\n",
    "        embedding_model_name (str):\n",
    "            The name of the embedding model e.g., `gte-large-en-v1.5`, etc.   If `model_serving_endpoint` is an OpenAI External Model or FMAPI model and set to `None`, this will be automatically detected.\n",
    "        chunk_size_tokens (int):\n",
    "            An optional size for each chunk in tokens. Defaults to `None`, which uses the model's entire context window.\n",
    "        chunk_overlap_token (int):\n",
    "            Tokens that should overlap between chunks. Defaults to `0`.\n",
    "\n",
    "    Returns:\n",
    "        A callable that takes a document (`str`) and produces a list of chunks (`list[str]`).\n",
    "    \"\"\"\n",
    "    \n",
    "    # imports here to prevent needing to install everywhere\n",
    "\n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "    from transformers import AutoTokenizer\n",
    "    import tiktoken\n",
    "\n",
    "    try:\n",
    "        # Detect the embedding model and its configuration\n",
    "        embedding_model_name, chunk_spec = detect_fmapi_embedding_model_type(\n",
    "            model_serving_endpoint\n",
    "        )\n",
    "\n",
    "        if chunk_spec is None or embedding_model_name is None:\n",
    "            # Fall back to using provided embedding_model_name\n",
    "            chunk_spec = EMBEDDING_MODELS.get(embedding_model_name)\n",
    "            if chunk_spec is None:\n",
    "                raise KeyError\n",
    "\n",
    "        # Update chunk specification based on provided parameters\n",
    "        chunk_spec[\"chunk_size_tokens\"] = (\n",
    "            chunk_size_tokens or chunk_spec[\"context_window\"]\n",
    "        )\n",
    "        chunk_spec[\"chunk_overlap_tokens\"] = chunk_overlap_tokens\n",
    "\n",
    "        # Validate chunk size and overlap\n",
    "        is_valid, msg = validate_chunk_size(chunk_spec)\n",
    "        if not is_valid:\n",
    "            raise ValueError(msg)\n",
    "        else:\n",
    "            print(msg)\n",
    "\n",
    "    except KeyError:\n",
    "        raise ValueError(\n",
    "            f\"Embedding model `{embedding_model_name}` not found. Available models: {EMBEDDING_MODELS.keys()}\"\n",
    "        )\n",
    "\n",
    "    def _recursive_character_text_splitter(text: str) -> list[str]:\n",
    "        tokenizer = chunk_spec[\"tokenizer\"]()\n",
    "        if chunk_spec[\"type\"] == \"SENTENCE_TRANSFORMER\":\n",
    "            splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "                tokenizer,\n",
    "                chunk_size=chunk_spec[\"chunk_size_tokens\"],\n",
    "                chunk_overlap=chunk_spec[\"chunk_overlap_tokens\"],\n",
    "            )\n",
    "        elif chunk_spec[\"type\"] == \"OPENAI\":\n",
    "            splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "                tokenizer.name,\n",
    "                chunk_size=chunk_spec[\"chunk_size_tokens\"],\n",
    "                chunk_overlap=chunk_spec[\"chunk_overlap_tokens\"],\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {chunk_spec['type']}\")\n",
    "        return splitter.split_text(text)\n",
    "\n",
    "    return _recursive_character_text_splitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "368b3588-ed1b-43e4-9003-bbe28041eb12",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define chunking config class"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.errors.platform import ResourceDoesNotExist\n",
    "from databricks.sdk.service.serving import EndpointStateReady\n",
    "\n",
    "\n",
    "class RecursiveTextSplitterChunkingConfig(SerializableConfig):\n",
    "    \"\"\"\n",
    "    Configuration for the Unstructured Data Pipeline.\n",
    "\n",
    "    Args:\n",
    "        embedding_model_endpoint (str):\n",
    "            Embedding model endpoint hosted on Model Serving.  Default is `databricks-gte-large`.  This can be an External Model, such as OpenAI, or a Databricks-hosted model on Foundational Model API. The list of Databricks-hosted models can be found here: https://docs.databricks.com/en/machine-learning/foundation-models/index.html\n",
    "        chunk_size_tokens (int):\n",
    "            The size of each chunk of the document in tokens. Default is 1024.\n",
    "        chunk_overlap_tokens (int):\n",
    "            The overlap of tokens between chunks. Default is 256.\n",
    "    \"\"\"\n",
    "\n",
    "    embedding_model_endpoint: str = \"databricks-gte-large-en\"\n",
    "    chunk_size_tokens: int = 1024\n",
    "    chunk_overlap_tokens: int = 256\n",
    "\n",
    "    def validate_embedding_endpoint(self) -> tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Validates that the specified embedding endpoint exists and is of the correct type\n",
    "        Returns:\n",
    "            tuple[bool, str]: A tuple containing (success, error_message).\n",
    "            If validation passes, returns (True, success_message). If validation fails, returns (False, error_message).\n",
    "        \"\"\"\n",
    "        task_type = \"llm/v1/embeddings\"\n",
    "        w = WorkspaceClient()\n",
    "        browser_url = get_workspace_hostname()\n",
    "        try:\n",
    "            llm_endpoint = w.serving_endpoints.get(name=self.embedding_model_endpoint)\n",
    "        except ResourceDoesNotExist as e:\n",
    "            msg = f\"Model serving endpoint {self.embedding_model_endpoint} not found.\"\n",
    "            return (False, msg)\n",
    "        if llm_endpoint.state.ready != EndpointStateReady.READY:\n",
    "            msg = f\"Model serving endpoint {self.embedding_model_endpoint} is not in a READY state.  Please visit the status page to debug: {browser_url}/ml/endpoints/{self.embedding_model_endpoint}\"\n",
    "            return (False, msg)\n",
    "        if llm_endpoint.task != task_type:\n",
    "            msg = f\"Model serving endpoint {self.embedding_model_endpoint} is online & ready, but does not support task type {task_type}.  Details at: {browser_url}/ml/endpoints/{self.embedding_model_endpoint}\"\n",
    "            return (False, msg)\n",
    "\n",
    "        msg = f\"Validated serving endpoint {self.embedding_model_endpoint} as READY and of type {task_type}.  View here: {browser_url}/ml/endpoints/{self.embedding_model_endpoint}\"\n",
    "        print(msg)\n",
    "        return (True, msg)\n",
    "\n",
    "    def validate_chunk_size_and_overlap(self) -> tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Validates that chunk_size and overlap values are valid\n",
    "        Returns:\n",
    "            tuple[bool, str]: A tuple containing (success, error_message).\n",
    "            If validation passes, returns (True, success_message). If validation fails, returns (False, error_message).\n",
    "        \"\"\"\n",
    "        # Detect the embedding model and its configuration\n",
    "        embedding_model_name, chunk_spec = detect_fmapi_embedding_model_type(\n",
    "            self.embedding_model_endpoint\n",
    "        )\n",
    "\n",
    "        # Update chunk specification based on provided parameters\n",
    "        chunk_spec[\"chunk_size_tokens\"] = self.chunk_size_tokens\n",
    "        chunk_spec[\"chunk_overlap_tokens\"] = self.chunk_overlap_tokens\n",
    "\n",
    "        if chunk_spec is None or embedding_model_name is None:\n",
    "            # Fall back to using provided embedding_model_name\n",
    "            chunk_spec = EMBEDDING_MODELS.get(embedding_model_name)\n",
    "            if chunk_spec is None:\n",
    "                msg = f\"Embedding model `{embedding_model_name}` not found, so can't validate chunking config. Chunking config must be validated for a specific embedding model.  Available models: {EMBEDDING_MODELS.keys()}\"\n",
    "                return (False, msg)\n",
    "\n",
    "        # Validate chunk size and overlap\n",
    "        is_valid, msg = validate_chunk_size(chunk_spec)\n",
    "        if not is_valid:\n",
    "            return (False, msg)\n",
    "        else:\n",
    "            print(msg)\n",
    "            return (True, msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06ee684b-c7bd-4c0e-8fd8-f54416948a5a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configure and validate a chunking config instance"
    }
   },
   "outputs": [],
   "source": [
    "chunking_config = RecursiveTextSplitterChunkingConfig(\n",
    "    embedding_model_endpoint=\"databricks-gte-large-en\",  # A Model Serving endpoint supporting the /llm/v1/embeddings task\n",
    "    chunk_size_tokens=1024,\n",
    "    chunk_overlap_tokens=256,\n",
    ")\n",
    "\n",
    "# Validate the embedding endpoint & chunking config\n",
    "is_valid, msg = chunking_config.validate_embedding_endpoint()\n",
    "if not is_valid:\n",
    "    raise Exception(msg)\n",
    "\n",
    "is_valid, msg = chunking_config.validate_chunk_size_and_overlap()\n",
    "if not is_valid:\n",
    "    raise Exception(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34eeeb8b-f232-4c46-8350-e227934c2d4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸš« âœï¸ Write the data pipeline configuration to a YAML\n",
    "\n",
    "The following cells define a consolidated configuration object and write out an instance of it to a file so that it can be reloaded later by other components. For instance, this allows the configuration to be loaded and referenced by the Agent's notebook. You would want to move this class definition to a separate Python file in your code path and the refer to the same module by both your data pipeline and your agent, as demonstrated in the [GenAI cookbook](https://github.com/databricks/genai-cookbook/tree/main/openai_sdk_agent_app_sample_code). We include the class inline here simply for ease of use in having a single notebook to show an end-to-end pipeline for learning purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1020efa6-f50a-4226-a4d7-bb5defafd315",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define overall pipeline config"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "\n",
    "class DataPipelineConfig(SerializableConfig):\n",
    "    source: UCVolumeSourceConfig\n",
    "    output: DataPipelineOutputConfig\n",
    "    chunking_config: RecursiveTextSplitterChunkingConfig\n",
    "\n",
    "    def model_dump(self, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Override model_dump to exclude name and description fields.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: Dictionary representation of the model excluding name and description.\n",
    "        \"\"\"\n",
    "        model_dumped = super().model_dump(**kwargs)\n",
    "        model_dumped[\"source\"] = yaml.safe_load(\n",
    "            serializable_config_to_yaml(self.source)\n",
    "        )\n",
    "        model_dumped[\"output\"] = yaml.safe_load(\n",
    "            serializable_config_to_yaml(self.output)\n",
    "        )\n",
    "        model_dumped[\"chunking_config\"] = yaml.safe_load(\n",
    "            serializable_config_to_yaml(self.chunking_config)\n",
    "        )\n",
    "        return model_dumped\n",
    "\n",
    "    @classmethod\n",
    "    def _load_class_from_dict(\n",
    "        cls, class_object, data: Dict[str, Any]\n",
    "    ) -> \"SerializableConfig\":\n",
    "        # Deserialize sub-configs\n",
    "        data[\"source\"] = load_serializable_config_from_yaml(yaml.dump(data[\"source\"]))\n",
    "        data[\"output\"] = load_serializable_config_from_yaml(yaml.dump(data[\"output\"]))\n",
    "        data[\"chunking_config\"] = load_serializable_config_from_yaml(\n",
    "            yaml.dump(data[\"chunking_config\"])\n",
    "        )\n",
    "        return class_object(**data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c88d58f0-e37b-45c8-82c0-49184dad41fb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create and save an instance of the pipeline config"
    }
   },
   "outputs": [],
   "source": [
    "data_pipeline_config = DataPipelineConfig(\n",
    "    source=source_config,\n",
    "    output=output_config,\n",
    "    chunking_config=chunking_config,\n",
    ")\n",
    "\n",
    "serializable_config_to_yaml_file(data_pipeline_config, \"./data_pipeline_config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a28cbf99-c4ca-4adc-905a-e7ebfe015730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ›‘ Pause - end of config section\n",
    "\n",
    "If you are running your initial data pipeline, you do not need to configure anything else, you can just `Run All` the notebook cells before.  You can modify these cells later to tune the quality of your data pipeline by changing the parsing logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95b6971b-b00b-4f42-bbe8-cc64eea2fff8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# âŒ¨ï¸ Data pipeline code\n",
    "\n",
    "The code below executes the data pipeline.  You can modify the below code as indicated to implement different parsing or chunking strategies or to extract additional metadata fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c85ddc92-10c5-405c-ae78-8ded5462333e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## âœ… âœï¸ Step 1: Load and parse documents into a Delta table\n",
    "\n",
    "In this step, we'll load files from the Unity Catalog Volume defined in `source_config` into the Delta table `storage_config.parsed_docs_table` . The contents of each file will become a separate row in our Delta table.\n",
    "\n",
    "The path to the source document will be used as the `doc_uri`, which is displayed to your end users in the Agent Evaluation web application.\n",
    "\n",
    "After you evaluate the outputs and test your POC with stakeholders, you can return here to change the parsing logic or extraction.\n",
    "\n",
    "**Customize the parsing function**\n",
    "\n",
    "This default implementation parses PDF, HTML, and DOCX files using open source libraries. The first cells below define the parsing logic and its return value. If needed after your initial evaluation, Databricks suggest modifying the parsing logic to add support for more file types or extracting additional metadata about each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "794f42f4-733a-429a-9836-711d53d8f268",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define the file parsing logic"
    }
   },
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import traceback\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# PDF libraries\n",
    "import fitz\n",
    "import pymupdf4llm\n",
    "\n",
    "# HTML libraries\n",
    "import markdownify\n",
    "import re\n",
    "\n",
    "## DOCX libraries\n",
    "import pypandoc\n",
    "import tempfile\n",
    "\n",
    "## JSON libraries\n",
    "import json\n",
    "\n",
    "\n",
    "# Schema of the dict returned by `file_parser(...)`\n",
    "# This is used to create the output Delta Table's schema.\n",
    "# Adjust the class if you want to add additional columns from your parser, such as extracting custom metadata.\n",
    "class ParserReturnValue(TypedDict):\n",
    "    # DO NOT CHANGE THESE NAMES\n",
    "    # Parsed content of the document\n",
    "    content: str  # do not change this name\n",
    "    # The status of whether the parser succeeds or fails, used to exclude failed files downstream\n",
    "    parser_status: str  # do not change this name\n",
    "    # Unique ID of the document\n",
    "    doc_uri: str  # do not change this name\n",
    "\n",
    "    # OK TO CHANGE THESE NAMES\n",
    "    # Optionally, you can add additional metadata fields here\n",
    "    # example_metadata: str\n",
    "    last_modified: datetime\n",
    "\n",
    "\n",
    "# Parser function.  Adjust this function to modify the parsing logic.\n",
    "def file_parser(\n",
    "    raw_doc_contents_bytes: bytes,\n",
    "    doc_path: str,\n",
    "    modification_time: datetime,\n",
    "    doc_bytes_length: int,\n",
    ") -> ParserReturnValue:\n",
    "    \"\"\"\n",
    "    Parses the content of a PDF document into a string.\n",
    "\n",
    "    This function takes the raw bytes of a PDF document and its path, attempts to parse the document using PyPDF,\n",
    "    and returns the parsed content and the status of the parsing operation.\n",
    "\n",
    "    Parameters:\n",
    "    - raw_doc_contents_bytes (bytes): The raw bytes of the document to be parsed (set by Spark when loading the file)\n",
    "    - doc_path (str): The DBFS path of the document, used to verify the file extension (set by Spark when loading the file)\n",
    "    - modification_time (timestamp): The last modification time of the document (set by Spark when loading the file)\n",
    "    - doc_bytes_length (long): The size of the document in bytes (set by Spark when loading the file)\n",
    "\n",
    "    Returns:\n",
    "    - ParserReturnValue: A dictionary containing the parsed document content and the status of the parsing operation.\n",
    "      The 'contenty will contain the parsed text as a string, and the 'parser_status' key will indicate\n",
    "      whether the parsing was successful or if an error occurred.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from markdownify import markdownify as md\n",
    "\n",
    "        filename, file_extension = os.path.splitext(doc_path)\n",
    "\n",
    "        if file_extension == \".pdf\":\n",
    "            pdf_doc = fitz.Document(stream=raw_doc_contents_bytes, filetype=\"pdf\")\n",
    "            md_text = pymupdf4llm.to_markdown(pdf_doc)\n",
    "\n",
    "            parsed_document = {\n",
    "                \"content\": md_text.strip(),\n",
    "                \"parser_status\": \"SUCCESS\",\n",
    "            }\n",
    "        elif file_extension == \".html\":\n",
    "            html_content = raw_doc_contents_bytes.decode(\"utf-8\")\n",
    "\n",
    "            markdown_contents = md(\n",
    "                str(html_content).strip(), heading_style=markdownify.ATX\n",
    "            )\n",
    "            markdown_stripped = re.sub(r\"\\n{3,}\", \"\\n\\n\", markdown_contents.strip())\n",
    "\n",
    "            parsed_document = {\n",
    "                \"content\": markdown_stripped,\n",
    "                \"parser_status\": \"SUCCESS\",\n",
    "            }\n",
    "        elif file_extension == \".docx\":\n",
    "            with tempfile.NamedTemporaryFile(delete=True) as temp_file:\n",
    "                temp_file.write(raw_doc_contents_bytes)\n",
    "                temp_file_path = temp_file.name\n",
    "                md = pypandoc.convert_file(temp_file_path, \"markdown\", format=\"docx\")\n",
    "\n",
    "                parsed_document = {\n",
    "                    \"content\": md.strip(),\n",
    "                    \"parser_status\": \"SUCCESS\",\n",
    "                }\n",
    "        elif file_extension in [\".txt\", \".md\"]:\n",
    "            parsed_document = {\n",
    "                \"content\": raw_doc_contents_bytes.decode(\"utf-8\").strip(),\n",
    "                \"parser_status\": \"SUCCESS\",\n",
    "            }\n",
    "        elif file_extension in [\".json\", \".jsonl\"]:\n",
    "            # NOTE: This is a placeholder for a JSON parser.  It's not a \"real\" parser, it just returns the raw JSON formatted into XML-like strings that LLMs tend to like.\n",
    "            json_data = json.loads(raw_doc_contents_bytes.decode(\"utf-8\"))\n",
    "\n",
    "            def flatten_json_to_xml(obj, parent_key=\"\"):\n",
    "                xml_parts = []\n",
    "                if isinstance(obj, dict):\n",
    "                    for key, value in obj.items():\n",
    "                        if isinstance(value, (dict, list)):\n",
    "                            xml_parts.append(flatten_json_to_xml(value, key))\n",
    "                        else:\n",
    "                            xml_parts.append(f\"<{key}>{str(value)}</{key}>\")\n",
    "                elif isinstance(obj, list):\n",
    "                    for i, item in enumerate(obj):\n",
    "                        if isinstance(item, (dict, list)):\n",
    "                            xml_parts.append(\n",
    "                                flatten_json_to_xml(item, f\"{parent_key}_{i}\")\n",
    "                            )\n",
    "                        else:\n",
    "                            xml_parts.append(\n",
    "                                f\"<{parent_key}_{i}>{str(item)}</{parent_key}_{i}>\"\n",
    "                            )\n",
    "                else:\n",
    "                    xml_parts.append(f\"<{parent_key}>{str(obj)}</{parent_key}>\")\n",
    "                return \"\\n\".join(xml_parts)\n",
    "\n",
    "            flattened_content = flatten_json_to_xml(json_data)\n",
    "            parsed_document = {\n",
    "                \"content\": flattened_content.strip(),\n",
    "                \"parser_status\": \"SUCCESS\",\n",
    "            }\n",
    "        else:\n",
    "            raise Exception(f\"No supported parser for {doc_path}\")\n",
    "\n",
    "        # Extract the required doc_uri\n",
    "        # convert from `dbfs:/Volumes/catalog/schema/pdf_docs/filename.pdf` to `/Volumes/catalog/schema/pdf_docs/filename.pdf`\n",
    "        modified_path = urlparse(doc_path).path\n",
    "        parsed_document[\"doc_uri\"] = modified_path\n",
    "\n",
    "        # Sample metadata extraction logic\n",
    "        # if \"test\" in parsed_document[\"content\n",
    "        #     parsed_document[\"example_metadata\"] = \"test\"\n",
    "        # else:\n",
    "        #     parsed_document[\"example_metadata\"] = \"not test\"\n",
    "\n",
    "        # Add the modified time\n",
    "        parsed_document[\"last_modified\"] = modification_time\n",
    "\n",
    "        return parsed_document\n",
    "\n",
    "    except Exception as e:\n",
    "        status = f\"An error occurred: {e}\\n{traceback.format_exc()}\"\n",
    "        warnings.warn(status)\n",
    "        return {\n",
    "            \"content\": \"\",\n",
    "            \"parser_status\": f\"ERROR: {status}\",\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3bac3bd-40ec-427c-9c78-1d2194cc682a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define logic to apply the parsing function"
    }
   },
   "outputs": [],
   "source": [
    "import traceback\n",
    "from datetime import datetime\n",
    "from typing import Any, Callable, TypedDict, Dict\n",
    "import os\n",
    "from IPython.display import display_markdown\n",
    "import warnings\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "\n",
    "\n",
    "def _parse_and_extract(\n",
    "    raw_doc_contents_bytes: bytes,\n",
    "    modification_time: datetime,\n",
    "    doc_bytes_length: int,\n",
    "    doc_path: str,\n",
    "    parse_file_udf: Callable[[[dict, Any]], str],\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Parses raw bytes & extract metadata.\"\"\"\n",
    "    try:\n",
    "        # Run the parser\n",
    "        parser_output_dict = parse_file_udf(\n",
    "            raw_doc_contents_bytes=raw_doc_contents_bytes,\n",
    "            doc_path=doc_path,\n",
    "            modification_time=modification_time,\n",
    "            doc_bytes_length=doc_bytes_length,\n",
    "        )\n",
    "\n",
    "        if parser_output_dict.get(\"parser_status\") == \"SUCCESS\":\n",
    "            return parser_output_dict\n",
    "        else:\n",
    "            raise Exception(parser_output_dict.get(\"parser_status\"))\n",
    "\n",
    "    except Exception as e:\n",
    "        status = f\"An error occurred: {e}\\n{traceback.format_exc()}\"\n",
    "        warnings.warn(status)\n",
    "        return {\n",
    "            \"content\": \"\",\n",
    "            \"doc_uri\": doc_path,\n",
    "            \"parser_status\": status,\n",
    "        }\n",
    "\n",
    "\n",
    "def _get_parser_udf(\n",
    "    # extract_metadata_udf: Callable[[[dict, Any]], str],\n",
    "    parse_file_udf: Callable[[[dict, Any]], str],\n",
    "    spark_dataframe_schema: StructType,\n",
    "):\n",
    "    \"\"\"Gets the Spark UDF which will parse the files in parallel.\n",
    "\n",
    "    Arguments:\n",
    "      - extract_metadata_udf: A function that takes parsed content and extracts the metadata\n",
    "      - parse_file_udf: A function that takes the raw file and returns the parsed text.\n",
    "      - spark_dataframe_schema: The resulting schema of the document delta table\n",
    "    \"\"\"\n",
    "    # This UDF will load each file, parse the doc, and extract metadata.\n",
    "    parser_udf = func.udf(\n",
    "        lambda raw_doc_contents_bytes, modification_time, doc_bytes_length, doc_path: _parse_and_extract(\n",
    "            raw_doc_contents_bytes,\n",
    "            modification_time,\n",
    "            doc_bytes_length,\n",
    "            doc_path,\n",
    "            parse_file_udf,\n",
    "        ),\n",
    "        returnType=spark_dataframe_schema,\n",
    "        useArrow=True,\n",
    "    )\n",
    "    return parser_udf\n",
    "\n",
    "\n",
    "def load_files_to_df(spark: SparkSession, source_path: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Load files from a directory into a Spark DataFrame.\n",
    "    Each row in the DataFrame will contain the path, length, and content of the file; for more\n",
    "    details, see https://spark.apache.org/docs/latest/sql-data-sources-binaryFile.html\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Loading the raw files from {source_path}...\")\n",
    "    # Load the raw riles\n",
    "    raw_files_df = (\n",
    "        spark.read.format(\"binaryFile\")\n",
    "        .option(\"recursiveFileLookup\", \"true\")\n",
    "        .load(source_path)\n",
    "    )\n",
    "\n",
    "    # Check that files were present and loaded\n",
    "    if raw_files_df.count() == 0:\n",
    "        raise Exception(f\"`{source_path}` does not contain any files.\")\n",
    "\n",
    "    # display_markdown(\n",
    "    #     f\"### Found {raw_files_df.count()} files in {source_path}: \", raw=True\n",
    "    # )\n",
    "    # raw_files_df.display()\n",
    "    return raw_files_df\n",
    "\n",
    "\n",
    "def apply_parsing_fn(\n",
    "    raw_files_df: DataFrame,\n",
    "    parse_file_fn: Callable[[[dict, Any]], str],\n",
    "    parsed_df_schema: StructType,\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Apply a file-parsing UDF to a DataFrame whose rows correspond to file content/metadata loaded via\n",
    "    https://spark.apache.org/docs/latest/sql-data-sources-binaryFile.html\n",
    "    Returns a DataFrame with the parsed content and metadata.\n",
    "    \"\"\"\n",
    "    print(\n",
    "        f\"Applying parsing & metadata extraction to {raw_files_df.count()} files using Spark - this may take a long time if you have many documents...\"\n",
    "    )\n",
    "\n",
    "    parser_udf = _get_parser_udf(parse_file_fn, parsed_df_schema)\n",
    "\n",
    "    # Run the parsing\n",
    "    parsed_files_staging_df = raw_files_df.withColumn(\n",
    "        \"parsing\", parser_udf(\"content\", \"modificationTime\", \"length\", \"path\")\n",
    "    ).drop(\"content\")\n",
    "\n",
    "    # Filter for successfully parsed files\n",
    "    parsed_files_df = parsed_files_staging_df  # .filter(\n",
    "    #    parsed_files_staging_df.parsing.parser_status == \"SUCCESS\"\n",
    "    # )\n",
    "\n",
    "    # Change the schema to the resulting schema\n",
    "    resulting_fields = [field.name for field in parsed_df_schema.fields]\n",
    "\n",
    "    parsed_files_df = parsed_files_df.select(\n",
    "        *[func.col(f\"parsing.{field}\").alias(field) for field in resulting_fields]\n",
    "    )\n",
    "    return parsed_files_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59d3927a-14af-435a-ada0-1a0946beb0d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The cell below contains debugging code to test your parsing function on a single record. This is a good place to iterate as you adjust the parsing logic above to see how your changes impact the parser output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48a3ab67-2e30-4e39-b05e-3a8ff304fd5b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Test the parsing logic on a few records"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "raw_files_df = load_files_to_df(\n",
    "    spark=spark,\n",
    "    source_path=source_config.volume_path,\n",
    ")\n",
    "print(f\"Loaded {raw_files_df.count()} files from {source_config.volume_path}.  Files: {source_config.list_files()}\")\n",
    "\n",
    "test_records_dict = raw_files_df.toPandas().to_dict(orient=\"records\")\n",
    "\n",
    "for record in test_records_dict:\n",
    "    print()\n",
    "    print(\"Testing parsing for file: \", record[\"path\"])\n",
    "    print()\n",
    "    test_result = file_parser(raw_doc_contents_bytes=record['content'], doc_path=record['path'], modification_time=record['modificationTime'], doc_bytes_length=record['length'])\n",
    "    print(test_result)\n",
    "    break # pause after 1 file.  if you want to test more files, remove the break statement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fb6db6c-faa0-4dac-be84-a832bbbb49b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸš«âœï¸ The below cell is boilerplate code to apply the parsing function using Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9905cc2c-ddc0-4ba2-b3df-89f27f8e6b7e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define some helpers for creating the DataFrame"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    DoubleType,\n",
    "    BooleanType,\n",
    "    ArrayType,\n",
    "    TimestampType,\n",
    "    DateType,\n",
    ")\n",
    "from typing import TypedDict, get_type_hints, List\n",
    "from datetime import datetime, date, time\n",
    "\n",
    "\n",
    "def typed_dict_to_spark_fields(typed_dict: type[TypedDict]) -> StructType:\n",
    "    \"\"\"\n",
    "    Converts a TypedDict into a list of Spark StructField objects.\n",
    "\n",
    "    This function maps Python types defined in a TypedDict to their corresponding\n",
    "    Spark SQL data types, facilitating the creation of a Spark DataFrame schema\n",
    "    from Python type annotations.\n",
    "\n",
    "    Parameters:\n",
    "    - typed_dict (type[TypedDict]): The TypedDict class to be converted.\n",
    "\n",
    "    Returns:\n",
    "    - StructType: A list of StructField objects representing the Spark schema.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If an unsupported type is encountered or if dictionary types are used.\n",
    "    \"\"\"\n",
    "\n",
    "    # Mapping of type names to Spark type objects\n",
    "    type_mapping = {\n",
    "        str: StringType(),\n",
    "        int: IntegerType(),\n",
    "        float: DoubleType(),\n",
    "        bool: BooleanType(),\n",
    "        list: ArrayType(StringType()),  # Default to StringType for arrays\n",
    "        datetime: TimestampType(),\n",
    "        date: DateType(),\n",
    "    }\n",
    "\n",
    "    def get_spark_type(value_type):\n",
    "        \"\"\"\n",
    "        Helper function to map a Python type to a Spark SQL data type.\n",
    "\n",
    "        This function supports basic Python types, lists of a single type, and raises\n",
    "        an error for unsupported types or dictionaries.\n",
    "\n",
    "        Parameters:\n",
    "        - value_type: The Python type to be converted.\n",
    "\n",
    "        Returns:\n",
    "        - DataType: The corresponding Spark SQL data type.\n",
    "\n",
    "        Raises:\n",
    "        - ValueError: If the type is unsupported or if dictionary types are used.\n",
    "        \"\"\"\n",
    "        if value_type in type_mapping:\n",
    "            return type_mapping[value_type]\n",
    "        elif hasattr(value_type, \"__origin__\") and value_type.__origin__ == list:\n",
    "            # Handle List[type] types\n",
    "            return ArrayType(get_spark_type(value_type.__args__[0]))\n",
    "        elif hasattr(value_type, \"__origin__\") and value_type.__origin__ == dict:\n",
    "            # Handle Dict[type, type] types (not fully supported)\n",
    "            raise ValueError(\"Dict types are not fully supported\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported type: {value_type}\")\n",
    "\n",
    "    # Get the type hints for the TypedDict\n",
    "    type_hints = get_type_hints(typed_dict)\n",
    "\n",
    "    # Convert the type hints into a list of StructField objects\n",
    "    fields = [\n",
    "        StructField(key, get_spark_type(value), True)\n",
    "        for key, value in type_hints.items()\n",
    "    ]\n",
    "\n",
    "    # Create and return the StructType object\n",
    "    return fields\n",
    "\n",
    "\n",
    "def typed_dicts_to_spark_schema(*typed_dicts: type[TypedDict]) -> StructType:\n",
    "    \"\"\"\n",
    "    Converts multiple TypedDicts into a Spark schema.\n",
    "\n",
    "    This function allows for the combination of multiple TypedDicts into a single\n",
    "    Spark DataFrame schema, enabling the creation of complex data structures.\n",
    "\n",
    "    Parameters:\n",
    "    - *typed_dicts: Variable number of TypedDict classes to be converted.\n",
    "\n",
    "    Returns:\n",
    "    - StructType: A Spark schema represented as a StructType object, which is a collection\n",
    "      of StructField objects derived from the provided TypedDicts.\n",
    "    \"\"\"\n",
    "    fields = []\n",
    "    for typed_dict in typed_dicts:\n",
    "        fields.extend(typed_dict_to_spark_fields(typed_dict))\n",
    "\n",
    "    return StructType(fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "165706b2-5824-42e7-a22b-3ca0edfd0a77",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Parse all the documents and write the table"
    }
   },
   "outputs": [],
   "source": [
    "# Tune this parameter to optimize performance.  \n",
    "# More partitions will improve performance, but may cause out of \n",
    "# memory errors if your cluster is too small.\n",
    "NUM_PARTITIONS = 50\n",
    "\n",
    "# Load the Unity Catalog Volume files into a Spark DataFrame\n",
    "raw_files_df = load_files_to_df(\n",
    "    spark=spark,\n",
    "    source_path=source_config.volume_path,\n",
    ").repartition(NUM_PARTITIONS)\n",
    "\n",
    "# Apply the parsing UDF to the Spark DataFrame\n",
    "parsed_files_df = apply_parsing_fn(\n",
    "    raw_files_df=raw_files_df,\n",
    "    # Modify this function to change the parser, extract additional metadata, etc\n",
    "    parse_file_fn=file_parser,\n",
    "    # The schema of the resulting Delta Table will follow the schema defined in ParserReturnValue\n",
    "    parsed_df_schema=typed_dicts_to_spark_schema(ParserReturnValue),\n",
    ")\n",
    "\n",
    "# Write to a Delta Table\n",
    "parsed_files_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\n",
    "    output_config.parsed_docs_table\n",
    ")\n",
    "\n",
    "# Get resulting table\n",
    "parsed_files_df = spark.table(output_config.parsed_docs_table)\n",
    "parsed_files_no_errors_df = parsed_files_df.filter(\n",
    "    parsed_files_df.parser_status == \"SUCCESS\"\n",
    ")\n",
    "\n",
    "# Show successfully parsed documents\n",
    "print(f\"Parsed {parsed_files_df.count()} / {parsed_files_no_errors_df.count()} documents successfully.  Inspect `parsed_files_no_errors_df` or visit {get_table_url(output_config.parsed_docs_table)} to see all parsed documents, including any errors.\")\n",
    "#display(parsed_files_no_errors_df.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9ab1d50-280d-4799-9e50-8d0519cba38e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Show any parsing failures or successfully parsed files that resulted in an empty document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6810e2d-7b0a-4a2f-9d22-5a00056bba4b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define helpers to check the parsed documents"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def check_parsed_df_for_errors(parsed_files_df) -> tuple[bool, str, DataFrame]:\n",
    "    # Check and warn on any errors\n",
    "    errors_df = parsed_files_df.filter(func.col(f\"parser_status\") != \"SUCCESS\")\n",
    "\n",
    "    num_errors = errors_df.count()\n",
    "    if num_errors > 0:\n",
    "        msg = f\"{num_errors} documents ({round(errors_df.count()/parsed_files_df.count(), 2)*100}) of documents had parse errors. Please review.\"\n",
    "        return (True, msg, errors_df)\n",
    "    else:\n",
    "        msg = \"All documents were parsed.\"\n",
    "        print(msg)\n",
    "        return (False, msg, None)\n",
    "\n",
    "\n",
    "def check_parsed_df_for_empty_parsed_files(parsed_files_df):\n",
    "    # Check and warn on any errors\n",
    "    num_empty_df = parsed_files_df.filter(\n",
    "        func.col(f\"parser_status\") == \"SUCCESS\"\n",
    "    ).filter(func.col(\"content\") == \"\")\n",
    "\n",
    "    num_errors = num_empty_df.count()\n",
    "    if num_errors > 0:\n",
    "        msg = f\"{num_errors} documents ({round(num_empty_df.count()/parsed_files_df.count(), 2)*100}) of documents returned empty parsing results. Please review.\"\n",
    "        return (True, msg, num_empty_df)\n",
    "    else:\n",
    "        msg = \"All documents produced non-null parsing results.\"\n",
    "        print(msg)\n",
    "        return (False, msg, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d548ce6-87ef-4cb3-8f7b-6dc6b7837d98",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check the results of the parsed output"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Any documents that failed to parse\n",
    "is_error, msg, failed_docs_df = check_parsed_df_for_errors(parsed_files_df)\n",
    "if is_error:\n",
    "    display(failed_docs_df.toPandas())\n",
    "    raise Exception(msg)\n",
    "    \n",
    "# Any documents that returned empty parsing results\n",
    "is_error, msg, empty_docs_df = check_parsed_df_for_empty_parsed_files(parsed_files_df)\n",
    "if is_error:\n",
    "    display(empty_docs_df.toPandas())\n",
    "    raise Exception(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e21c84e8-7682-4a7a-86fc-7f4f990bb490",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## âœ… âœï¸ Step 2: Compute chunks of documents\n",
    "\n",
    "In this step, we will split our documents into smaller chunks to index them in our vector database.\n",
    "\n",
    "We provide a default implementation of a recursive text splitter.  To create your own chunking logic, adapt the `get_recursive_character_text_splitter()` function defined in one of the prior cells which is called in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02c40228-f933-4af8-9121-ed2efa0985dd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create the chunking function"
    }
   },
   "outputs": [],
   "source": [
    "# Get the chunking function\n",
    "recursive_character_text_splitter_fn = get_recursive_character_text_splitter(\n",
    "    model_serving_endpoint=chunking_config.embedding_model_endpoint,\n",
    "    chunk_size_tokens=chunking_config.chunk_size_tokens,\n",
    "    chunk_overlap_tokens=chunking_config.chunk_overlap_tokens,\n",
    ")\n",
    "\n",
    "# Determine which columns to propagate from the docs table to the chunks table.\n",
    "\n",
    "# Get the columns from the parser except for the content\n",
    "# You can modify this to adjust which fields are propagated from the docs table to the chunks table.\n",
    "propagate_columns = [\n",
    "    field.name\n",
    "    for field in typed_dicts_to_spark_schema(ParserReturnValue).fields\n",
    "    if field.name != \"content\"\n",
    "]\n",
    "\n",
    "# If you want to implement retrieval strategies such as presenting the entire document vs. the chunk to the LLM, include `contentich contains the doc's full parsed text.  By default this is not included because the size of contcontentquite large and cause performance issues.\n",
    "# propagate_columns = [\n",
    "#     field.name\n",
    "#     for field in typed_dicts_to_spark_schema(ParserReturnValue).fields\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ca333a2-a64b-46e3-af4e-d54409a8331b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define a helper to apply chunking"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Literal, Optional, Any, Callable\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from pyspark.sql.functions import explode\n",
    "import pyspark.sql.functions as func\n",
    "from typing import Callable\n",
    "from pyspark.sql.types import StructType, StringType, StructField, MapType, ArrayType\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "\n",
    "\n",
    "def apply_chunking_fn(\n",
    "    parsed_docs_df: DataFrame,\n",
    "    chunking_fn: Callable[[str], list[str]],\n",
    "    propagate_columns: list[str],\n",
    "    doc_column: str = \"content\",\n",
    ") -> DataFrame:\n",
    "    # imports here to avoid requiring these libraries in all notebooks since the data pipeline config imports this package\n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "    from transformers import AutoTokenizer\n",
    "    import tiktoken\n",
    "\n",
    "    print(\n",
    "        f\"Applying chunking UDF to {parsed_docs_df.count()} documents using Spark - this may take a long time if you have many documents...\"\n",
    "    )\n",
    "\n",
    "    parser_udf = func.udf(\n",
    "        chunking_fn, returnType=ArrayType(StringType()), useArrow=True\n",
    "    )\n",
    "    chunked_array_docs = parsed_docs_df.withColumn(\n",
    "        \"content_chunked\", parser_udf(doc_column)\n",
    "    )  # .drop(doc_column)\n",
    "    chunked_docs = chunked_array_docs.select(\n",
    "        *propagate_columns, explode(\"content_chunked\").alias(\"content_chunked\")\n",
    "    )\n",
    "\n",
    "    # Add a primary key: \"chunk_id\".\n",
    "    chunks_with_ids = chunked_docs.withColumn(\n",
    "        \"chunk_id\", func.md5(func.col(\"content_chunked\"))\n",
    "    )\n",
    "    # Reorder for better display.\n",
    "    chunks_with_ids = chunks_with_ids.select(\n",
    "        \"chunk_id\", \"content_chunked\", *propagate_columns\n",
    "    )\n",
    "\n",
    "    return chunks_with_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "189933e6-d3cf-470f-845b-8759ec42382a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸš«âœï¸ Run the chunking function within Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dfa90f8-c4dc-4485-8fa8-dcd4c7d40618",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Chunk all the parsed documents"
    }
   },
   "outputs": [],
   "source": [
    "# Set the TRANSFORMERS_CACHE environment variable to a writable directory\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/dbfs/tmp/transformers_cache'\n",
    "\n",
    "# Tune this parameter to optimize performance.  More partitions will improve performance, but may cause out of memory errors if your cluster is too small.\n",
    "NUM_PARTITIONS = 50\n",
    "\n",
    "# Load parsed docs\n",
    "parsed_files_df = spark.table(output_config.parsed_docs_table).repartition(NUM_PARTITIONS)\n",
    "\n",
    "chunked_docs_df = chunked_docs_table = apply_chunking_fn(\n",
    "    # The source documents table.\n",
    "    parsed_docs_df=parsed_files_df,\n",
    "    # The chunking function that takes a string (document) and returns a list of strings (chunks).\n",
    "    chunking_fn=recursive_character_text_splitter_fn,\n",
    "    # Choose which columns to propagate from the docs table to chunks table. `doc_uri` column is required we can propagate the original document URL to the Agent's web app.\n",
    "    propagate_columns=propagate_columns,\n",
    ")\n",
    "\n",
    "# Write to Delta Table\n",
    "chunked_docs_df.write.mode(\"overwrite\").option(\n",
    "    \"overwriteSchema\", \"true\"\n",
    ").saveAsTable(output_config.chunked_docs_table)\n",
    "\n",
    "# Get resulting table\n",
    "chunked_docs_df = spark.table(output_config.chunked_docs_table)\n",
    "\n",
    "# Show number of chunks created\n",
    "print(f\"Created {chunked_docs_df.count()} chunks.  Inspect `chunked_docs_df` or visit {get_table_url(output_config.chunked_docs_table)} to see the results.\")\n",
    "\n",
    "# Enable CDC feed for VS index sync\n",
    "cdc_results = spark.sql(f\"ALTER TABLE {output_config.chunked_docs_table} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n",
    "\n",
    "# Show chunks\n",
    "display(chunked_docs_df.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fe923a8-89c2-4852-9cea-98074b3ce404",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸš« âœï¸ Step 3: Create the vector index\n",
    "\n",
    "In this step, we'll embed the documents to compute the vector index over the chunks and create our retriever index that will be used to query relevant documents to the user question.  The embedding pipeline is handled within Databricks Vector Search using [Delta Sync](https://docs.databricks.com/en/generative-ai/create-query-vector-search.html#create-a-vector-search-index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd30af32-fbb5-4114-a924-54d5b7d03e05",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define a helper to build the index"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk.service.vectorsearch import (\n",
    "    VectorSearchIndexesAPI,\n",
    "    DeltaSyncVectorIndexSpecRequest,\n",
    "    EmbeddingSourceColumn,\n",
    "    PipelineType,\n",
    "    VectorIndexType,\n",
    ")\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.errors.platform import ResourceDoesNotExist, BadRequest\n",
    "import time\n",
    "\n",
    "# `build_retriever_index` will build the vector search index which is used by our RAG to retrieve relevant documents.\n",
    "\n",
    "# Arguments:\n",
    "# - `chunked_docs_table`: The chunked documents table. There is expected to be a `chunked_text` column, a `chunk_id` column, and a `url` column.\n",
    "# -  `primary_key`: The column to use for the vector index primary key.\n",
    "# - `embedding_source_column`: The column to compute embeddings for in the vector index.\n",
    "# - `vector_search_endpoint`: An optional vector search endpoint name. It not defined, defaults to the `{table_id}_vector_search`.\n",
    "# - `vector_search_index_name`: An optional index name. If not defined, defaults to `{chunked_docs_table}_index`.\n",
    "# - `embedding_endpoint_name`: An embedding endpoint name.\n",
    "# - `force_delete_vector_search_endpoint`: Setting this to true will rebuild the vector search endpoint.\n",
    "\n",
    "\n",
    "def build_retriever_index(\n",
    "    vector_search_endpoint: str,\n",
    "    chunked_docs_table_name: str,\n",
    "    vector_search_index_name: str,\n",
    "    embedding_endpoint_name: str,\n",
    "    force_delete_index_before_create=False,\n",
    "    primary_key: str = \"chunk_id\",  # hard coded in the apply_chunking_fn\n",
    "    embedding_source_column: str = \"content_chunked\",  # hard coded in the apply_chunking_fn\n",
    ") -> tuple[bool, str]:\n",
    "    # Initialize workspace client and vector search API\n",
    "    w = WorkspaceClient()\n",
    "    vsc = w.vector_search_indexes\n",
    "\n",
    "    def find_index(index_name):\n",
    "        try:\n",
    "            return vsc.get_index(index_name=index_name)\n",
    "        except ResourceDoesNotExist:\n",
    "            return None\n",
    "\n",
    "    def wait_for_index_to_be_ready(index):\n",
    "        while not index.status.ready:\n",
    "            print(\n",
    "                f\"Index {vector_search_index_name} exists, but is not ready, waiting 30 seconds...\"\n",
    "            )\n",
    "            time.sleep(30)\n",
    "            index = find_index(index_name=vector_search_index_name)\n",
    "\n",
    "    def wait_for_index_to_be_deleted(index):\n",
    "        while index:\n",
    "            print(\n",
    "                f\"Waiting for index {vector_search_index_name} to be deleted, waiting 30 seconds...\"\n",
    "            )\n",
    "            time.sleep(30)\n",
    "            index = find_index(index_name=vector_search_index_name)\n",
    "\n",
    "    existing_index = find_index(index_name=vector_search_index_name)\n",
    "    if existing_index:\n",
    "        print(f\"Found existing index {get_table_url(vector_search_index_name)}...\")\n",
    "        if force_delete_index_before_create:\n",
    "            print(f\"Deleting index {vector_search_index_name}...\")\n",
    "            vsc.delete_index(index_name=vector_search_index_name)\n",
    "            wait_for_index_to_be_deleted(existing_index)\n",
    "            create_index = True\n",
    "        else:\n",
    "            wait_for_index_to_be_ready(existing_index)\n",
    "            create_index = False\n",
    "            print(\n",
    "                f\"Starting the sync of index {vector_search_index_name}, this can take 15 minutes or much longer if you have a larger number of documents.\"\n",
    "            )\n",
    "            # print(existing_index)\n",
    "            try:\n",
    "                vsc.sync_index(index_name=vector_search_index_name)\n",
    "                msg = f\"Kicked off index sync for {vector_search_index_name}.\"\n",
    "                return (False, msg)\n",
    "            except BadRequest as e:\n",
    "                msg = f\"Index sync already in progress, so failed to kick off index sync for {vector_search_index_name}.  Please wait for the index to finish syncing and try again.\"\n",
    "                return (True, msg)\n",
    "    else:\n",
    "        print(\n",
    "            f'Creating new vector search index \"{vector_search_index_name}\" on endpoint \"{vector_search_endpoint}\"'\n",
    "        )\n",
    "        create_index = True\n",
    "\n",
    "    if create_index:\n",
    "        print(\n",
    "            \"Computing document embeddings and Vector Search Index. This can take 15 minutes or much longer if you have a larger number of documents.\"\n",
    "        )\n",
    "        try:\n",
    "            # Create delta sync index spec using the proper class\n",
    "            delta_sync_spec = DeltaSyncVectorIndexSpecRequest(\n",
    "                source_table=chunked_docs_table_name,\n",
    "                pipeline_type=PipelineType.TRIGGERED,\n",
    "                embedding_source_columns=[\n",
    "                    EmbeddingSourceColumn(\n",
    "                        name=embedding_source_column,\n",
    "                        embedding_model_endpoint_name=embedding_endpoint_name,\n",
    "                    )\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            vsc.create_index(\n",
    "                name=vector_search_index_name,\n",
    "                endpoint_name=vector_search_endpoint,\n",
    "                primary_key=primary_key,\n",
    "                index_type=VectorIndexType.DELTA_SYNC,\n",
    "                delta_sync_index_spec=delta_sync_spec,\n",
    "            )\n",
    "            msg = (\n",
    "                f\"Successfully created vector search index {vector_search_index_name}.\"\n",
    "            )\n",
    "            print(msg)\n",
    "            return (False, msg)\n",
    "        except Exception as e:\n",
    "            msg = f\"Vector search index creation failed. Wait 5 minutes and try running this cell again.\"\n",
    "            return (True, msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d53faa42-2a65-40b0-8fc1-6c27e88df6d0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Build the index"
    }
   },
   "outputs": [],
   "source": [
    "is_error, msg = retriever_index_result = build_retriever_index(\n",
    "    # Spark requires `` to escape names with special chars, VS client does not.\n",
    "    chunked_docs_table_name=output_config.chunked_docs_table.replace(\"`\", \"\"),\n",
    "    vector_search_endpoint=output_config.vector_search_endpoint,\n",
    "    vector_search_index_name=output_config.vector_index,\n",
    "\n",
    "    # Must match the embedding endpoint you used to chunk your documents\n",
    "    embedding_endpoint_name=chunking_config.embedding_model_endpoint,\n",
    "\n",
    "    # Set to true to re-create the vector search endpoint when re-running the data pipeline.  If set to True, syncing will not work if re-run the pipeline and change the schema of chunked_docs_table_name.  Keeping this as False will allow Vector Search to avoid recomputing embeddings for any row with that has a chunk_id that was previously computed.\n",
    "    force_delete_index_before_create=False,\n",
    ")\n",
    "if is_error:\n",
    "    raise Exception(msg)\n",
    "else:\n",
    "    print(\"NOTE: This cell will complete before the vector index has finished syncing/embedding your chunks & is ready for queries!\")\n",
    "    print(f\"View sync status here: {get_table_url(output_config.vector_index)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a1ad14b-2573-4485-8369-d417f7a548f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸš« âœï¸ Print links to view the resulting tables/index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cd40431-4cd3-4cc9-b38d-5ab817c40043",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Print links to the output artifacts in Unity Catalog"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Parsed docs table: {get_table_url(output_config.parsed_docs_table)}\\n\")\n",
    "print(f\"Chunked docs table: {get_table_url(output_config.chunked_docs_table)}\\n\")\n",
    "print(f\"Vector search index: {get_table_url(output_config.vector_index)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8908198917810236,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02. ICD10 unstructured ingest and transform",
   "widgets": {
    "catalog": {
     "currentValue": "danny_catalog",
     "nuid": "9fda58d6-f9ad-4589-924d-d36f89bfd9a5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Catalog",
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Catalog",
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "db_name": {
     "currentValue": "clinical_coding",
     "nuid": "0d84ad09-1bc3-440b-b76b-f09dd2e162ea",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Database",
      "name": "db_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Database",
      "name": "db_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "volume_name": {
     "currentValue": "icd10",
     "nuid": "d126cd24-24b4-497e-ab8e-ad0e23bd654e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Volume Name",
      "name": "volume_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Volume Name",
      "name": "volume_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "genai-cookbook-T2SdtsNM-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
